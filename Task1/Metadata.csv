title,authors,summary,comment,journal_ref,doi,entry_id,updated,published,primary_category,categories,links,pdf_url
Even $1 \times n$ Edge-Matching and Jigsaw Puzzles are Really Hard,"['Jeffrey Bosboom', 'Erik D. Demaine', 'Martin L. Demaine', 'Adam Hesterberg', 'Pasin Manurangsi', 'Anak Yodpinyanee']","We prove the computational intractability of rotating and placing $n$ square
tiles into a $1 \times n$ array such that adjacent tiles are compatible--either
equal edge colors, as in edge-matching puzzles, or matching tab/pocket shapes,
as in jigsaw puzzles. Beyond basic NP-hardness, we prove that it is NP-hard
even to approximately maximize the number of placed tiles (allowing blanks),
while satisfying the compatibility constraint between nonblank tiles, within a
factor of 0.9999999851. (On the other hand, there is an easy $1 \over
2$-approximation.) This is the first (correct) proof of inapproximability for
edge-matching and jigsaw puzzles. Along the way, we prove NP-hardness of
distinguishing, for a directed graph on $n$ nodes, between having a Hamiltonian
path (length $n-1$) and having at most $0.999999284 (n-1)$ edges that form a
vertex-disjoint union of paths. We use this gap hardness and gap-preserving
reductions to establish similar gap hardness for $1 \times n$ jigsaw and
edge-matching puzzles.","22 pages, 9 figures",,,http://arxiv.org/abs/1701.00146v1,2016-12-31 17:05:53+00:00,2016-12-31 17:05:53+00:00,cs.CC,"['cs.CC', 'cs.CG']","[arxiv.Result.Link('http://arxiv.org/abs/1701.00146v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1701.00146v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1701.00146v1
NIPS 2016 Tutorial: Generative Adversarial Networks,['Ian Goodfellow'],"This report summarizes the tutorial presented by the author at NIPS 2016 on
generative adversarial networks (GANs). The tutorial describes: (1) Why
generative modeling is a topic worth studying, (2) how generative models work,
and how GANs compare to other generative models, (3) the details of how GANs
work, (4) research frontiers in GANs, and (5) state-of-the-art image models
that combine GANs with other methods. Finally, the tutorial contains three
exercises for readers to complete, and the solutions to these exercises.",v2-v4 are all typo fixes. No substantive changes relative to v1,,,http://arxiv.org/abs/1701.00160v4,2017-04-03 21:57:48+00:00,2016-12-31 19:17:17+00:00,cs.LG,['cs.LG'],"[arxiv.Result.Link('http://arxiv.org/abs/1701.00160v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1701.00160v4', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1701.00160v4
Classification of Smartphone Users Using Internet Traffic,"['Andrey Finkelstein', 'Ron Biton', 'Rami Puzis', 'Asaf Shabtai']","Today, smartphone devices are owned by a large portion of the population and
have become a very popular platform for accessing the Internet. Smartphones
provide the user with immediate access to information and services. However,
they can easily expose the user to many privacy risks. Applications that are
installed on the device and entities with access to the device's Internet
traffic can reveal private information about the smartphone user and steal
sensitive content stored on the device or transmitted by the device over the
Internet. In this paper, we present a method to reveal various demographics and
technical computer skills of smartphone users by their Internet traffic
records, using machine learning classification models. We implement and
evaluate the method on real life data of smartphone users and show that
smartphone users can be classified by their gender, smoking habits, software
programming experience, and other characteristics.",,,,http://arxiv.org/abs/1701.00220v1,2017-01-01 08:12:49+00:00,2017-01-01 08:12:49+00:00,cs.LG,"['cs.LG', 'cs.CR']","[arxiv.Result.Link('http://arxiv.org/abs/1701.00220v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1701.00220v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1701.00220v1
"Closed Sets and Operators thereon: Representations, Computability and Complexity",['Carsten Rösnick-Neugebauer'],"The TTE approach to Computable Analysis is the study of so-called
representations (encodings for continuous objects such as reals, functions, and
sets) with respect to the notions of computability they induce. A rich variety
of such representations had been devised over the past decades, particularly
regarding closed subsets of Euclidean space plus subclasses thereof (like
compact subsets). In addition, they had been compared and classified with
respect to both non-uniform computability of single sets and uniform
computability of operators on sets. In this paper we refine these
investigations from the point of view of computational complexity. Benefiting
from the concept of second-order representations and complexity recently
devised by Kawamura & Cook (2012), we determine parameterized complexity bounds
for operators such as union, intersection, projection, and more generally
function image and inversion. By indicating natural parameters in addition to
the output precision, we get a uniform view on results by Ko (1991-2013),
Braverman (2004/05) and Zhao & M\""uller (2008), relating these problems to the
P/UP/NP question in discrete complexity theory.",,"Logical Methods in Computer Science, Volume 14, Issue 2 (April 10,
  2018) lmcs:4432",10.23638/LMCS-14(2:1)2018,http://arxiv.org/abs/1701.00227v4,2018-04-09 07:07:33+00:00,2017-01-01 11:25:42+00:00,cs.CC,"['cs.CC', 'cs.LO', 'math.LO', '03D15', 'F.4.1']","[arxiv.Result.Link('http://dx.doi.org/10.23638/LMCS-14(2:1)2018', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1701.00227v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1701.00227v4', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1701.00227v4
Outlier Robust Online Learning,"['Jiashi Feng', 'Huan Xu', 'Shie Mannor']","We consider the problem of learning from noisy data in practical settings
where the size of data is too large to store on a single machine. More
challenging, the data coming from the wild may contain malicious outliers. To
address the scalability and robustness issues, we present an online robust
learning (ORL) approach. ORL is simple to implement and has provable robustness
guarantee -- in stark contrast to existing online learning approaches that are
generally fragile to outliers. We specialize the ORL approach for two concrete
cases: online robust principal component analysis and online linear regression.
We demonstrate the efficiency and robustness advantages of ORL through
comprehensive simulations and predicting image tags on a large-scale data set.
We also discuss extension of the ORL to distributed learning and provide
experimental evaluations.",,,,http://arxiv.org/abs/1701.00251v1,2017-01-01 15:18:13+00:00,2017-01-01 15:18:13+00:00,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/1701.00251v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1701.00251v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1701.00251v1
STRIPS Planning in Infinite Domains,"['Caelan Reed Garrett', 'Tomás Lozano-Pérez', 'Leslie Pack Kaelbling']","Many robotic planning applications involve continuous actions with highly
non-linear constraints, which cannot be modeled using modern planners that
construct a propositional representation. We introduce STRIPStream: an
extension of the STRIPS language which can model these domains by supporting
the specification of blackbox generators to handle complex constraints. The
outputs of these generators interact with actions through possibly infinite
streams of objects and static predicates. We provide two algorithms which both
reduce STRIPStream problems to a sequence of finite-domain planning problems.
The representation and algorithms are entirely domain independent. We
demonstrate our framework on simple illustrative domains, and then on a
high-dimensional, continuous robotic task and motion planning domain.",11 pages,,,http://arxiv.org/abs/1701.00287v2,2017-05-28 01:08:00+00:00,2017-01-01 20:37:51+00:00,cs.AI,['cs.AI'],"[arxiv.Result.Link('http://arxiv.org/abs/1701.00287v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1701.00287v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1701.00287v2
Dynamic Deep Neural Networks: Optimizing Accuracy-Efficiency Trade-offs by Selective Execution,"['Lanlan Liu', 'Jia Deng']","We introduce Dynamic Deep Neural Networks (D2NN), a new type of feed-forward
deep neural network that allows selective execution. Given an input, only a
subset of D2NN neurons are executed, and the particular subset is determined by
the D2NN itself. By pruning unnecessary computation depending on input, D2NNs
provide a way to improve computational efficiency. To achieve dynamic selective
execution, a D2NN augments a feed-forward deep neural network (directed acyclic
graph of differentiable modules) with controller modules. Each controller
module is a sub-network whose output is a decision that controls whether other
modules can execute. A D2NN is trained end to end. Both regular and controller
modules in a D2NN are learnable and are jointly trained to optimize both
accuracy and efficiency. Such training is achieved by integrating
backpropagation with reinforcement learning. With extensive experiments of
various D2NN architectures on image classification tasks, we demonstrate that
D2NNs are general and flexible, and can effectively optimize
accuracy-efficiency trade-offs.","fixed typos; updated CIFAR-10 results and added more details;
  corrected the cascade D2NN configuration details",,,http://arxiv.org/abs/1701.00299v3,2018-03-05 02:03:00+00:00,2017-01-02 00:09:14+00:00,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/1701.00299v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1701.00299v3', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1701.00299v3
An affective computational model for machine consciousness,['Rohitash Chandra'],"In the past, several models of consciousness have become popular and have led
to the development of models for machine consciousness with varying degrees of
success and challenges for simulation and implementations. Moreover, affective
computing attributes that involve emotions, behavior and personality have not
been the focus of models of consciousness as they lacked motivation for
deployment in software applications and robots. The affective attributes are
important factors for the future of machine consciousness with the rise of
technologies that can assist humans. Personality and affection hence can give
an additional flavor for the computational model of consciousness in humanoid
robotics. Recent advances in areas of machine learning with a focus on deep
learning can further help in developing aspects of machine consciousness in
areas that can better replicate human sensory perceptions such as speech
recognition and vision. With such advancements, one encounters further
challenges in developing models that can synchronize different aspects of
affective computing. In this paper, we review some existing models of
consciousnesses and present an affective computational model that would enable
the human touch and feel for robotic systems.",under review,,,http://arxiv.org/abs/1701.00349v1,2017-01-02 09:48:47+00:00,2017-01-02 09:48:47+00:00,cs.AI,['cs.AI'],"[arxiv.Result.Link('http://arxiv.org/abs/1701.00349v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1701.00349v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1701.00349v1
Conceptual Spaces for Cognitive Architectures: A Lingua Franca for Different Levels of Representation,"['Antonio Lieto', 'Antonio Chella', 'Marcello Frixione']","During the last decades, many cognitive architectures (CAs) have been
realized adopting different assumptions about the organization and the
representation of their knowledge level. Some of them (e.g. SOAR [Laird
(2012)]) adopt a classical symbolic approach, some (e.g. LEABRA [O'Reilly and
Munakata (2000)]) are based on a purely connectionist model, while others (e.g.
CLARION [Sun (2006)] adopt a hybrid approach combining connectionist and
symbolic representational levels. Additionally, some attempts (e.g. biSOAR)
trying to extend the representational capacities of CAs by integrating
diagrammatical representations and reasoning are also available [Kurup and
Chandrasekaran (2007)]. In this paper we propose a reflection on the role that
Conceptual Spaces, a framework developed by Peter G\""ardenfors [G\""ardenfors
(2000)] more than fifteen years ago, can play in the current development of the
Knowledge Level in Cognitive Systems and Architectures. In particular, we claim
that Conceptual Spaces offer a lingua franca that allows to unify and
generalize many aspects of the symbolic, sub-symbolic and diagrammatic
approaches (by overcoming some of their typical problems) and to integrate them
on a common ground. In doing so we extend and detail some of the arguments
explored by G\""ardenfors [G\""ardenfors (1997)] for defending the need of a
conceptual, intermediate, representation level between the symbolic and the
sub-symbolic one.","31 pages, 3 figures in Biologically Inspired Cognitive Architectures,
  2017",,10.1016/j.bica.2016.10.005,http://arxiv.org/abs/1701.00464v1,2017-01-02 17:35:34+00:00,2017-01-02 17:35:34+00:00,cs.AI,['cs.AI'],"[arxiv.Result.Link('http://dx.doi.org/10.1016/j.bica.2016.10.005', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1701.00464v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1701.00464v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1701.00464v1
Two-Bit Networks for Deep Learning on Resource-Constrained Embedded Devices,"['Wenjia Meng', 'Zonghua Gu', 'Ming Zhang', 'Zhaohui Wu']","With the rapid proliferation of Internet of Things and intelligent edge
devices, there is an increasing need for implementing machine learning
algorithms, including deep learning, on resource-constrained mobile embedded
devices with limited memory and computation power. Typical large Convolutional
Neural Networks (CNNs) need large amounts of memory and computational power,
and cannot be deployed on embedded devices efficiently. We present Two-Bit
Networks (TBNs) for model compression of CNNs with edge weights constrained to
(-2, -1, 1, 2), which can be encoded with two bits. Our approach can reduce the
memory usage and improve computational efficiency significantly while achieving
good performance in terms of classification accuracy, thus representing a
reasonable tradeoff between model size and performance.",,,,http://arxiv.org/abs/1701.00485v2,2017-01-04 13:54:51+00:00,2017-01-02 04:28:16+00:00,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Link('http://arxiv.org/abs/1701.00485v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1701.00485v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1701.00485v2
f-Divergence constrained policy improvement,"['Boris Belousov', 'Jan Peters']","To ensure stability of learning, state-of-the-art generalized policy
iteration algorithms augment the policy improvement step with a trust region
constraint bounding the information loss. The size of the trust region is
commonly determined by the Kullback-Leibler (KL) divergence, which not only
captures the notion of distance well but also yields closed-form solutions. In
this paper, we consider a more general class of f-divergences and derive the
corresponding policy update rules. The generic solution is expressed through
the derivative of the convex conjugate function to f and includes the KL
solution as a special case. Within the class of f-divergences, we further focus
on a one-parameter family of $\alpha$-divergences to study effects of the
choice of divergence on policy improvement. Previously known as well as new
policy updates emerge for different values of $\alpha$. We show that every type
of policy update comes with a compatible policy evaluation resulting from the
chosen f-divergence. Interestingly, the mean-squared Bellman error minimization
is closely related to policy evaluation with the Pearson $\chi^2$-divergence
penalty, while the KL divergence results in the soft-max policy update and a
log-sum-exp critic. We carry out asymptotic analysis of the solutions for
different values of $\alpha$ and demonstrate the effects of using different
divergence functions on a multi-armed bandit problem and on common standard
reinforcement learning problems.",,,,http://arxiv.org/abs/1801.00056v2,2018-04-04 11:36:12+00:00,2017-12-29 23:07:26+00:00,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/1801.00056v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.00056v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1801.00056v2
Parameter-free online learning via model selection,"['Dylan J. Foster', 'Satyen Kale', 'Mehryar Mohri', 'Karthik Sridharan']","We introduce an efficient algorithmic framework for model selection in online
learning, also known as parameter-free online learning. Departing from previous
work, which has focused on highly structured function classes such as nested
balls in Hilbert space, we propose a generic meta-algorithm framework that
achieves online model selection oracle inequalities under minimal structural
assumptions. We give the first computationally efficient parameter-free
algorithms that work in arbitrary Banach spaces under mild smoothness
assumptions; previous results applied only to Hilbert spaces. We further derive
new oracle inequalities for matrix classes, non-nested convex sets, and
$\mathbb{R}^{d}$ with generic regularizers. Finally, we generalize these
results by providing oracle inequalities for arbitrary non-linear classes in
the online supervised learning model. These results are all derived through a
unified meta-algorithm scheme using a novel ""multi-scale"" algorithm for
prediction with expert advice based on random playout, which may be of
independent interest.",NIPS 2017,,,http://arxiv.org/abs/1801.00101v2,2018-01-03 22:25:05+00:00,2017-12-30 08:21:19+00:00,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/1801.00101v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.00101v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1801.00101v2
PAC-Bayesian Margin Bounds for Convolutional Neural Networks,"['Konstantinos Pitas', 'Mike Davies', 'Pierre Vandergheynst']","Recently the generalization error of deep neural networks has been analyzed
through the PAC-Bayesian framework, for the case of fully connected layers. We
adapt this approach to the convolutional setting.",arXiv admin note: text overlap with arXiv:1707.09564 by other authors,,,http://arxiv.org/abs/1801.00171v2,2018-04-20 09:27:47+00:00,2017-12-30 18:11:59+00:00,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/1801.00171v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.00171v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1801.00171v2
Theory of Deep Learning III: explaining the non-overfitting puzzle,"['Tomaso Poggio', 'Kenji Kawaguchi', 'Qianli Liao', 'Brando Miranda', 'Lorenzo Rosasco', 'Xavier Boix', 'Jack Hidary', 'Hrushikesh Mhaskar']","A main puzzle of deep networks revolves around the absence of overfitting
despite large overparametrization and despite the large capacity demonstrated
by zero training error on randomly labeled data. In this note, we show that the
dynamics associated to gradient descent minimization of nonlinear networks is
topologically equivalent, near the asymptotically stable minima of the
empirical error, to linear gradient system in a quadratic potential with a
degenerate (for square loss) or almost degenerate (for logistic or crossentropy
loss) Hessian. The proposition depends on the qualitative theory of dynamical
systems and is supported by numerical results. Our main propositions extend to
deep nonlinear networks two properties of gradient descent for linear networks,
that have been recently established (1) to be key to their generalization
properties: 1. Gradient descent enforces a form of implicit regularization
controlled by the number of iterations, and asymptotically converges to the
minimum norm solution for appropriate initial conditions of gradient descent.
This implies that there is usually an optimum early stopping that avoids
overfitting of the loss. This property, valid for the square loss and many
other loss functions, is relevant especially for regression. 2. For
classification, the asymptotic convergence to the minimum norm solution implies
convergence to the maximum margin solution which guarantees good classification
error for ""low noise"" datasets. This property holds for loss functions such as
the logistic and cross-entropy loss independently of the initial conditions.
The robustness to overparametrization has suggestive implications for the
robustness of the architecture of deep convolutional networks with respect to
the curse of dimensionality.",,,,http://arxiv.org/abs/1801.00173v2,2018-01-16 08:54:12+00:00,2017-12-30 18:27:35+00:00,cs.LG,['cs.LG'],"[arxiv.Result.Link('http://arxiv.org/abs/1801.00173v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.00173v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1801.00173v2
Deep Reinforcement Learning for List-wise Recommendations,"['Xiangyu Zhao', 'Liang Zhang', 'Long Xia', 'Zhuoye Ding', 'Dawei Yin', 'Jiliang Tang']","Recommender systems play a crucial role in mitigating the problem of
information overload by suggesting users' personalized items or services. The
vast majority of traditional recommender systems consider the recommendation
procedure as a static process and make recommendations following a fixed
strategy. In this paper, we propose a novel recommender system with the
capability of continuously improving its strategies during the interactions
with users. We model the sequential interactions between users and a
recommender system as a Markov Decision Process (MDP) and leverage
Reinforcement Learning (RL) to automatically learn the optimal strategies via
recommending trial-and-error items and receiving reinforcements of these items
from users' feedbacks. In particular, we introduce an online user-agent
interacting environment simulator, which can pre-train and evaluate model
parameters offline before applying the model online. Moreover, we validate the
importance of list-wise recommendations during the interactions between users
and agent, and develop a novel approach to incorporate them into the proposed
framework LIRD for list-wide recommendations. The experimental results based on
a real-world e-commerce dataset demonstrate the effectiveness of the proposed
framework.",,,,http://arxiv.org/abs/1801.00209v3,2019-06-27 06:29:27+00:00,2017-12-30 23:30:36+00:00,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/1801.00209v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.00209v3', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1801.00209v3
Game-theoretic Network Centrality: A Review,"['Mateusz K. Tarkowski', 'Tomasz P. Michalak', 'Talal Rahwan', 'Michael Wooldridge']","Game-theoretic centrality is a flexible and sophisticated approach to
identify the most important nodes in a network. It builds upon the methods from
cooperative game theory and network theory. The key idea is to treat nodes as
players in a cooperative game, where the value of each coalition is determined
by certain graph-theoretic properties. Using solution concepts from cooperative
game theory, it is then possible to measure how responsible each node is for
the worth of the network.
  The literature on the topic is already quite large, and is scattered among
game-theoretic and computer science venues. We review the main game-theoretic
network centrality measures from both bodies of literature and organize them
into two categories: those that are more focused on the connectivity of nodes,
and those that are more focused on the synergies achieved by nodes in groups.
We present and explain each centrality, with a focus on algorithms and
complexity.",,,,http://arxiv.org/abs/1801.00218v1,2017-12-31 01:10:39+00:00,2017-12-31 01:10:39+00:00,cs.AI,"['cs.AI', 'cs.GT']","[arxiv.Result.Link('http://arxiv.org/abs/1801.00218v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.00218v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1801.00218v1
Using Deep Neural Network Approximate Bayesian Network,"['Jie Jia', 'Honggang Zhou', 'Yunchun Li']","We present a new method to approximate posterior probabilities of Bayesian
Network using Deep Neural Network. Experiment results on several public
Bayesian Network datasets shows that Deep Neural Network is capable of learning
joint probability distri- bution of Bayesian Network by learning from a few
observation and posterior probability distribution pairs with high accuracy.
Compared with traditional approximate method likelihood weighting sampling
algorithm, our method is much faster and gains higher accuracy in medium sized
Bayesian Network. Another advantage of our method is that our method can be
parallelled much easier in GPU without extra effort. We also ex- plored the
connection between the accuracy of our model and the number of training
examples. The result shows that our model saturate as the number of training
examples grow and we don't need many training examples to get reasonably good
result. Another contribution of our work is that we have shown discriminative
model like Deep Neural Network can approximate generative model like Bayesian
Network.",,,,http://arxiv.org/abs/1801.00282v2,2018-01-11 05:36:36+00:00,2017-12-31 13:26:20+00:00,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/1801.00282v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.00282v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1801.00282v2
Restricted Boltzmann Machines for Robust and Fast Latent Truth Discovery,"['Klaus Broelemann', 'Thomas Gottron', 'Gjergji Kasneci']","We address the problem of latent truth discovery, LTD for short, where the
goal is to discover the underlying true values of entity attributes in the
presence of noisy, conflicting or incomplete information. Despite a multitude
of algorithms to address the LTD problem that can be found in literature, only
little is known about their overall performance with respect to effectiveness
(in terms of truth discovery capabilities), efficiency and robustness. A
practical LTD approach should satisfy all these characteristics so that it can
be applied to heterogeneous datasets of varying quality and degrees of
cleanliness.
  We propose a novel algorithm for LTD that satisfies the above requirements.
The proposed model is based on Restricted Boltzmann Machines, thus coined
LTD-RBM. In extensive experiments on various heterogeneous and publicly
available datasets, LTD-RBM is superior to state-of-the-art LTD techniques in
terms of an overall consideration of effectiveness, efficiency and robustness.",,,,http://arxiv.org/abs/1801.00283v1,2017-12-31 13:26:51+00:00,2017-12-31 13:26:51+00:00,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/1801.00283v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.00283v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1801.00283v1
ZOOpt: Toolbox for Derivative-Free Optimization,"['Yu-Ren Liu', 'Yi-Qi Hu', 'Hong Qian', 'Chao Qian', 'Yang Yu']","Recent advances in derivative-free optimization allow efficient approximation
of the global-optimal solutions of sophisticated functions, such as functions
with many local optima, non-differentiable and non-continuous functions. This
article describes the ZOOpt (Zeroth Order Optimization) toolbox that provides
efficient derivative-free solvers and is designed easy to use. ZOOpt provides
single-machine parallel optimization on the basis of python core and
multi-machine distributed optimization for time-consuming tasks by
incorporating with the Ray framework -- a famous platform for building
distributed applications. ZOOpt particularly focuses on optimization problems
in machine learning, addressing high-dimensional and noisy problems such as
hyper-parameter tuning and direct policy search. The toolbox is maintained
toward a ready-to-use tool in real-world machine learning tasks.","SCIENCE CHINA Information Sciences, 2022. Codes:
  https://github.com/polixir/ZOOpt","SCIENCE CHINA Information Sciences, 65: 207101, 2022",10.1007/s11432-021-3416-y,http://arxiv.org/abs/1801.00329v3,2022-06-02 02:10:52+00:00,2017-12-31 18:06:25+00:00,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://dx.doi.org/10.1007/s11432-021-3416-y', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1801.00329v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.00329v3', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1801.00329v3
Users Constraints in Itemset Mining,"['Christian Bessiere', 'Nadjib Lazaar', 'Yahia Lebbah', 'Mehdi Maamar']","Discovering significant itemsets is one of the fundamental problems in data
mining. It has recently been shown that constraint programming is a flexible
way to tackle data mining tasks. With a constraint programming approach, we can
easily express and efficiently answer queries with users constraints on items.
However, in many practical cases it is possible that queries also express users
constraints on the dataset itself. For instance, asking for a particular
itemset in a particular part of the dataset. This paper presents a general
constraint programming model able to handle any kind of query on the items or
the dataset for itemset mining.",,,,http://arxiv.org/abs/1801.00345v2,2018-02-08 16:21:54+00:00,2017-12-31 19:55:52+00:00,cs.AI,"['cs.AI', 'cs.DB']","[arxiv.Result.Link('http://arxiv.org/abs/1801.00345v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.00345v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1801.00345v2
SenseNet: 3D Objects Database and Tactile Simulator,['Jason Toy'],"The majority of artificial intelligence research, as it relates from which to
biological senses has been focused on vision. The recent explosion of machine
learning and in particular, dee p learning, can be partially attributed to the
release of high quality data sets for algorithm s from which to model the world
on. Thus, most of these datasets are comprised of images. We believe that
focusing on sensorimotor systems and tactile feedback will create algorithms
that better mimic human intelligence. Here we present SenseNet: a collection of
tactile simulators and a large scale dataset of 3D objects for manipulation.
SenseNet was created for the purpose of researching and training Artificial
Intelligences (AIs) to interact with the environment via sensorimotor neural
systems and tactile feedback. We aim to accelerate that same explosion in image
processing, but for the domain of tactile feedback and sensorimotor research.
We hope that SenseNet can offer researchers in both the machine learning and
computational neuroscience communities brand new opportunities and avenues to
explore.",,,,http://arxiv.org/abs/1801.00361v1,2017-12-31 21:50:15+00:00,2017-12-31 21:50:15+00:00,cs.AI,"['cs.AI', 'cs.RO']","[arxiv.Result.Link('http://arxiv.org/abs/1801.00361v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.00361v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1801.00361v1
Error-Robust Multi-View Clustering,"['Mehrnaz Najafi', 'Lifang He', 'Philip S. Yu']","In the era of big data, data may come from multiple sources, known as
multi-view data. Multi-view clustering aims at generating better clusters by
exploiting complementary and consistent information from multiple views rather
than relying on the individual view. Due to inevitable system errors caused by
data-captured sensors or others, the data in each view may be erroneous.
Various types of errors behave differently and inconsistently in each view.
More precisely, error could exhibit as noise and corruptions in reality.
Unfortunately, none of the existing multi-view clustering approaches handle all
of these error types. Consequently, their clustering performance is
dramatically degraded. In this paper, we propose a novel Markov chain method
for Error-Robust Multi-View Clustering (EMVC). By decomposing each view into a
shared transition probability matrix and error matrix and imposing structured
sparsity-inducing norms on error matrices, we characterize and handle typical
types of errors explicitly. To solve the challenging optimization problem, we
propose a new efficient algorithm based on Augmented Lagrangian Multipliers and
prove its convergence rigorously. Experimental results on various synthetic and
real-world datasets show the superiority of the proposed EMVC method over the
baseline methods and its robustness against different types of errors.","10 pages, 2017 IEEE International Conference on Big Data (Big Data
  2017)",,,http://arxiv.org/abs/1801.00384v1,2018-01-01 02:42:04+00:00,2018-01-01 02:42:04+00:00,cs.LG,['cs.LG'],"[arxiv.Result.Link('http://arxiv.org/abs/1801.00384v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.00384v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1801.00384v1
Theoretical Analysis of Sparse Subspace Clustering with Missing Entries,"['Manolis C. Tsakiris', 'Rene Vidal']","Sparse Subspace Clustering (SSC) is a popular unsupervised machine learning
method for clustering data lying close to an unknown union of low-dimensional
linear subspaces; a problem with numerous applications in pattern recognition
and computer vision. Even though the behavior of SSC for complete data is by
now well-understood, little is known about its theoretical properties when
applied to data with missing entries. In this paper we give theoretical
guarantees for SSC with incomplete data, and analytically establish that
projecting the zero-filled data onto the observation pattern of the point being
expressed leads to a substantial improvement in performance. The main insight
that stems from our analysis is that even though the projection induces
additional missing entries, this is counterbalanced by the fact that the
projected and zero-filled data are in effect incomplete points associated with
the union of the corresponding projected subspaces, with respect to which the
point being expressed is complete. The significance of this phenomenon
potentially extends to the entire class of self-expressive methods.",,"Proceedings of the 35th International Conference on Machine
  Learning, PMLR 80:4975-4984, 2018",,http://arxiv.org/abs/1801.00393v3,2018-02-09 17:27:43+00:00,2018-01-01 04:51:46+00:00,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/1801.00393v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.00393v3', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1801.00393v3
Auto-Generation of Pipelined Hardware Designs for Polar Encoder,"['Zhiwei Zhong', 'Xiaohu You', 'Chuan Zhang']","This paper presents a general framework for auto-generation of pipelined
polar encoder architectures. The proposed framework could be well represented
by a general formula. Given arbitrary code length $N$ and the level of
parallelism $M$, the formula could specify the corresponding hardware
architecture. We have written a compiler which could read the formula and then
automatically generate its register-transfer level (RTL) description suitable
for FPGA or ASIC implementation. With this hardware generation system, one
could explore the design space and make a trade-off between cost and
performance. Our experimental results have demonstrated the efficiency of this
auto-generator for polar encoder architectures.",,,,http://arxiv.org/abs/1801.00472v1,2018-01-01 16:50:09+00:00,2018-01-01 16:50:09+00:00,cs.AR,['cs.AR'],"[arxiv.Result.Link('http://arxiv.org/abs/1801.00472v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.00472v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1801.00472v1
Convex Relaxations of Convolutional Neural Nets,"['Burak Bartan', 'Mert Pilanci']","We propose convex relaxations for convolutional neural nets with one hidden
layer where the output weights are fixed. For convex activation functions such
as rectified linear units, the relaxations are convex second order cone
programs which can be solved very efficiently. We prove that the relaxation
recovers the global minimum under a planted model assumption, given
sufficiently many training samples from a Gaussian distribution. We also
identify a phase transition phenomenon in recovering the global minimum for the
relaxation.",,,,http://arxiv.org/abs/1901.00035v1,2018-12-31 20:21:48+00:00,2018-12-31 20:21:48+00:00,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/1901.00035v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00035v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.00035v1
Determining Principal Component Cardinality through the Principle of Minimum Description Length,['Ami Tavory'],"PCA (Principal Component Analysis) and its variants areubiquitous techniques
for matrix dimension reduction and reduced-dimensionlatent-factor extraction.
One significant challenge in using PCA, is thechoice of the number of principal
components. The information-theoreticMDL (Minimum Description Length) principle
gives objective compression-based criteria for model selection, but it is
difficult to analytically applyits modern definition - NML (Normalized Maximum
Likelihood) - to theproblem of PCA. This work shows a general reduction of NML
prob-lems to lower-dimension problems. Applying this reduction, it boundsthe
NML of PCA, by terms of the NML of linear regression, which areknown.",LOD 2019,,,http://arxiv.org/abs/1901.00059v2,2019-06-29 18:16:48+00:00,2018-12-31 22:41:32+00:00,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/1901.00059v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00059v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.00059v2
Impossibility and Uncertainty Theorems in AI Value Alignment (or why your AGI should not have a utility function),['Peter Eckersley'],"Utility functions or their equivalents (value functions, objective functions,
loss functions, reward functions, preference orderings) are a central tool in
most current machine learning systems. These mechanisms for defining goals and
guiding optimization run into practical and conceptual difficulty when there
are independent, multi-dimensional objectives that need to be pursued
simultaneously and cannot be reduced to each other. Ethicists have proved
several impossibility theorems that stem from this origin; those results appear
to show that there is no way of formally specifying what it means for an
outcome to be good for a population without violating strong human ethical
intuitions (in such cases, the objective function is a social welfare
function). We argue that this is a practical problem for any machine learning
system (such as medical decision support systems or autonomous weapons) or
rigidly rule-based bureaucracy that will make high stakes decisions about human
lives: such systems should not use objective functions in the strict
mathematical sense.
  We explore the alternative of using uncertain objectives, represented for
instance as partially ordered preferences, or as probability distributions over
total orders. We show that previously known impossibility theorems can be
transformed into uncertainty theorems in both of those settings, and prove
lower bounds on how much uncertainty is implied by the impossibility results.
We close by proposing two conjectures about the relationship between
uncertainty in objectives and severe unintended consequences from AI systems.","Published in SafeAI 2019: Proceedings of the AAAI Workshop on
  Artificial Intelligence Safety 2019",,,http://arxiv.org/abs/1901.00064v3,2019-03-05 03:12:49+00:00,2018-12-31 23:51:27+00:00,cs.AI,['cs.AI'],"[arxiv.Result.Link('http://arxiv.org/abs/1901.00064v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00064v3', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.00064v3
Recurrent Neural Networks for Time Series Forecasting,['Gábor Petneházi'],"Time series forecasting is difficult. It is difficult even for recurrent
neural networks with their inherent ability to learn sequentiality. This
article presents a recurrent neural network based time series forecasting
framework covering feature engineering, feature importances, point and interval
predictions, and forecast evaluation. The description of the method is followed
by an empirical study using both LSTM and GRU networks.",,,,http://arxiv.org/abs/1901.00069v1,2019-01-01 00:52:01+00:00,2019-01-01 00:52:01+00:00,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/1901.00069v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00069v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.00069v1
Exploring spectro-temporal features in end-to-end convolutional neural networks,"['Sean Robertson', 'Gerald Penn', 'Yingxue Wang']","Triangular, overlapping Mel-scaled filters (""f-banks"") are the current
standard input for acoustic models that exploit their input's time-frequency
geometry, because they provide a psycho-acoustically motivated time-frequency
geometry for a speech signal. F-bank coefficients are provably robust to small
deformations in the scale. In this paper, we explore two ways in which filter
banks can be adjusted for the purposes of speech recognition. First, triangular
filters can be replaced with Gabor filters, a compactly supported filter that
better localizes events in time, or Gammatone filters, a
psychoacoustically-motivated filter. Second, by rearranging the order of
operations in computing filter bank features, features can be integrated over
smaller time scales while simultaneously providing better frequency resolution.
We make all feature implementations available online through open-source
repositories. Initial experimentation with a modern end-to-end CNN phone
recognizer yielded no significant improvements to phone error rate due to
either modification. The result, and its ramifications with respect to learned
filter banks, is discussed.",,,,http://arxiv.org/abs/1901.00072v1,2019-01-01 01:17:26+00:00,2019-01-01 01:17:26+00:00,cs.LG,"['cs.LG', 'cs.CL', 'cs.SD', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/1901.00072v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00072v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.00072v1
High Performance GNR Power Gating for Low-Voltage CMOS Circuits,"['Hader E. El-hmaily', 'Rabab Ezz-Eldin', 'A. I. A. Galal', 'Hesham F. A. Hamed']","A robust power gating design using Graphene Nano-Ribbon Field Effect
Transistors (GNRFET) is proposed using 16nm technology. The Power Gating (PG)
structure is composed of GNRFET as a power switch and MOS power gated module.
The proposed structure resolves the main drawbacks of the traditional PG design
from the point of view increasing the propagation delay and wake-up time in low
voltage regions. GNRFET/MOSFET Conjunction (GMC) is employed to build various
structures of PG, GMCPG-SS and GMCPG-NS. In addition to exploiting it to build
two multi-mode PG structures. Circuit analysis for CMOS power gated logic
modules ISCAS85 benchmark of 16nm technology is used to evaluate the
performance of the proposed GNR power switch is compared to the traditional MOS
one. Leakage power, wake-up time and power delay product are used as
performance circuit parameters for the evaluation.","9 pages, 13 Figures and 5 Tables",,,http://arxiv.org/abs/1901.00092v1,2019-01-01 04:55:14+00:00,2019-01-01 04:55:14+00:00,cs.AR,"['cs.AR', '94C99', 'B.7.1']","[arxiv.Result.Link('http://arxiv.org/abs/1901.00092v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00092v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.00092v1
Morphological Network: How Far Can We Go with Morphological Neurons?,"['Ranjan Mondal', 'Sanchayan Santra', 'Soumendu Sundar Mukherjee', 'Bhabatosh Chanda']","Morphological neurons, that is morphological operators such as dilation and
erosion with learnable structuring elements, have intrigued researchers for
quite some time because of the power these operators bring to the table despite
their simplicity. These operators are known to be powerful nonlinear tools, but
for a given problem coming up with a sequence of operations and their
structuring element is a non-trivial task. So, the existing works have mainly
focused on this part of the problem without delving deep into their
applicability as generic operators. A few works have tried to utilize
morphological neurons as a part of classification (and regression) networks
when the input is a feature vector. However, these methods mainly focus on a
specific problem, without going into generic theoretical analysis. In this
work, we have theoretically analyzed morphological neurons and have shown that
these are far more powerful than previously anticipated. Our proposed
morphological block, containing dilation and erosion followed by their linear
combination, represents a sum of hinge functions. Existing works show that
hinge functions perform quite well in classification and regression problems.
Two morphological blocks can even approximate any continuous function. However,
to facilitate the theoretical analysis that we have done in this paper, we have
restricted ourselves to the 1D version of the operators, where the structuring
element operates on the whole input. Experimental evaluations also indicate the
effectiveness of networks built with morphological neurons, over similarly
structured neural networks.",Accepted at BMVC 2022,,,http://arxiv.org/abs/1901.00109v4,2022-12-14 04:48:24+00:00,2019-01-01 07:52:24+00:00,cs.LG,"['cs.LG', 'cs.CV', 'cs.NE', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/1901.00109v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00109v4', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.00109v4
An Active Learning Framework for Efficient Robust Policy Search,"['Sai Kiran Narayanaswami', 'Nandan Sudarsanam', 'Balaraman Ravindran']","Robust Policy Search is the problem of learning policies that do not degrade
in performance when subject to unseen environment model parameters. It is
particularly relevant for transferring policies learned in a simulation
environment to the real world. Several existing approaches involve sampling
large batches of trajectories which reflect the differences in various possible
environments, and then selecting some subset of these to learn robust policies,
such as the ones that result in the worst performance. We propose an active
learning based framework, EffAcTS, to selectively choose model parameters for
this purpose so as to collect only as much data as necessary to select such a
subset. We apply this framework using Linear Bandits, and experimentally
validate the gains in sample efficiency and the performance of our approach on
standard continuous control tasks. We also present a Multi-Task Learning
perspective to the problem of Robust Policy Search, and draw connections from
our proposed framework to existing work on Multi-Task Learning.","9 pages, 6 figures",,10.1145/3493700.3493712,http://arxiv.org/abs/1901.00117v2,2021-11-21 01:02:47+00:00,2019-01-01 08:50:47+00:00,cs.LG,"['cs.LG', 'cs.AI', 'cs.RO', 'stat.ML']","[arxiv.Result.Link('http://dx.doi.org/10.1145/3493700.3493712', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1901.00117v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00117v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.00117v2
Realizing data features by deep nets,"['Zheng-Chu Guo', 'Lei Shi', 'Shao-Bo Lin']","This paper considers the power of deep neural networks (deep nets for short)
in realizing data features. Based on refined covering number estimates, we find
that, to realize some complex data features, deep nets can improve the
performances of shallow neural networks (shallow nets for short) without
requiring additional capacity costs. This verifies the advantage of deep nets
in realizing complex features. On the other hand, to realize some simple data
feature like the smoothness, we prove that, up to a logarithmic factor, the
approximation rate of deep nets is asymptotically identical to that of shallow
nets, provided that the depth is fixed. This exhibits a limitation of deep nets
in realizing simple features.","12 pages, 2 figures",,,http://arxiv.org/abs/1901.00130v1,2019-01-01 11:00:44+00:00,2019-01-01 11:00:44+00:00,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/1901.00130v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00130v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.00130v1
A Theoretical Analysis of Deep Q-Learning,"['Jianqing Fan', 'Zhaoran Wang', 'Yuchen Xie', 'Zhuoran Yang']","Despite the great empirical success of deep reinforcement learning, its
theoretical foundation is less well understood. In this work, we make the first
attempt to theoretically understand the deep Q-network (DQN) algorithm (Mnih et
al., 2015) from both algorithmic and statistical perspectives. In specific, we
focus on a slight simplification of DQN that fully captures its key features.
Under mild assumptions, we establish the algorithmic and statistical rates of
convergence for the action-value functions of the iterative policy sequence
obtained by DQN. In particular, the statistical error characterizes the bias
and variance that arise from approximating the action-value function using deep
neural network, while the algorithmic error converges to zero at a geometric
rate. As a byproduct, our analysis provides justifications for the techniques
of experience replay and target network, which are crucial to the empirical
success of DQN. Furthermore, as a simple extension of DQN, we propose the
Minimax-DQN algorithm for zero-sum Markov game with two players. Borrowing the
analysis of DQN, we also quantify the difference between the policies obtained
by Minimax-DQN and the Nash equilibrium of the Markov game in terms of both the
algorithmic and statistical rates of convergence.",65 pages,,,http://arxiv.org/abs/1901.00137v3,2020-02-24 02:40:28+00:00,2019-01-01 11:41:23+00:00,cs.LG,"['cs.LG', 'math.OC', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/1901.00137v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00137v3', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.00137v3
Algorithmically Efficient Syntactic Characterization of Possibility Domains,"['Josep Díaz', 'Lefteris Kirousis', 'Sofia Kokonezi', 'John Livieratos']","In the field of Judgment Aggrgation, a domain, that is a subset of a
Cartesian power of $\{0,1\}$, is considered to reflect abstract rationality
restrictions on vectors of two-valued judgments on a number of issues. We are
interested in the ways we can aggregate the positions of a set of individuals,
whose positions over each issue form vectors of the domain, by means of
unanimous (idempotent) functions, whose output is again an element of the
domain. Such functions are called non-dictatorial, when their output is not
simply the positions of a single individual. Here, we consider domains
admitting various kinds of non-dictatorial aggregators, which reflect various
properties of majority aggregation: (locally) non-dictatorial, generalized
dictatorships, anonymous, monotone, StrongDem and systematic. We show that
interesting and, in some sense, democratic voting schemes are always provided
by domains that can be described by propositional formulas of specific
syntactic types we define. Furthermore, we show that we can efficiently
recognize such formulas and that, given a domain, we can both efficiently check
if it is described by such a formula and, in case it is, construct it. Our
results fall in the realm of classical results concerning the syntactic
characterization of domains with specific closure properties, like domains
closed under logical AND which are the models of Horn formulas. The techniques
we use to obtain our results draw from judgment aggregation as well as
propositional logic and universal algebra.",39 pages,,,http://arxiv.org/abs/1901.00138v3,2019-09-03 08:45:57+00:00,2019-01-01 11:45:59+00:00,cs.CC,['cs.CC'],"[arxiv.Result.Link('http://arxiv.org/abs/1901.00138v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00138v3', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.00138v3
On the Parameterized Cluster Editing with Vertex Splitting Problem,"['Faisal N. Abu-Khzam', 'Judith Egan', 'Serge Gaspers', 'Alexis Shaw', 'Peter Shaw']","In the Cluster Editing problem, a given graph is to be transformed into a
disjoint union of cliques via a minimum number of edge editing operations. In
this paper we introduce a new variant of Cluster Editing whereby a vertex can
be divided, or split, into two or more vertices thus allowing a single vertex
to belong to multiple clusters. This new problem, Cluster Editing with Vertex
Splitting, has applications in finding correlation clusters in discrete data,
including graphs obtained from Biological Network analysis. We initiate the
study of this new problem and show that it is fixed-parameter tractable when
parameterized by the total number of vertex splitting and edge editing
operations. In particular we obtain a 4k(k + 1) vertex kernel for the problem
and an $\Oh^*(2^{\Oh(k^2)})$ search algorithm.",,,,http://arxiv.org/abs/1901.00156v1,2019-01-01 14:28:31+00:00,2019-01-01 14:28:31+00:00,cs.CC,"['cs.CC', 'cs.DS']","[arxiv.Result.Link('http://arxiv.org/abs/1901.00156v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00156v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.00156v1
Supervised Multiscale Dimension Reduction for Spatial Interaction Networks,"['Shaobo Han', 'David B. Dunson']","We introduce a multiscale supervised dimension reduction method for SPatial
Interaction Network (SPIN) data, which consist of a collection of spatially
coordinated interactions. This type of predictor arises when the sampling unit
of data is composed of a collection of primitive variables, each of them being
essentially unique, so that it becomes necessary to group the variables in
order to simplify the representation and enhance interpretability. In this
paper, we introduce an empirical Bayes approach called spinlets, which first
constructs a partitioning tree to guide the reduction over multiple spatial
granularities, and then refines the representation of predictors according to
the relevance to the response. We consider an inverse Poisson regression model
and propose a new multiscale generalized double Pareto prior, which is induced
via a tree-structured parameter expansion scheme. Our approach is motivated by
an application in soccer analytics, in which we obtain compact vectorial
representations and readily interpretable visualizations of the complex network
objects, supervised by the response of interest.","30 pages, 12 figures, revised for clarity and conciseness",,,http://arxiv.org/abs/1901.00172v3,2019-06-08 20:28:55+00:00,2019-01-01 16:01:36+00:00,cs.LG,"['cs.LG', 'cs.SI', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/1901.00172v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00172v3', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.00172v3
Complementary reinforcement learning towards explainable agents,['Jung Hoon Lee'],"Reinforcement learning (RL) algorithms allow agents to learn skills and
strategies to perform complex tasks without detailed instructions or expensive
labelled training examples. That is, RL agents can learn, as we learn. Given
the importance of learning in our intelligence, RL has been thought to be one
of key components to general artificial intelligence, and recent breakthroughs
in deep reinforcement learning suggest that neural networks (NN) are natural
platforms for RL agents. However, despite the efficiency and versatility of
NN-based RL agents, their decision-making remains incomprehensible, reducing
their utilities. To deploy RL into a wider range of applications, it is
imperative to develop explainable NN-based RL agents. Here, we propose a method
to derive a secondary comprehensible agent from a NN-based RL agent, whose
decision-makings are based on simple rules. Our empirical evaluation of this
secondary agent's performance supports the possibility of building a
comprehensible and transparent agent using a NN-based RL agent.","14 pages, 5 figures",,,http://arxiv.org/abs/1901.00188v2,2019-01-24 02:29:24+00:00,2019-01-01 18:14:35+00:00,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/1901.00188v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00188v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.00188v2
Tighter Problem-Dependent Regret Bounds in Reinforcement Learning without Domain Knowledge using Value Function Bounds,"['Andrea Zanette', 'Emma Brunskill']","Strong worst-case performance bounds for episodic reinforcement learning
exist but fortunately in practice RL algorithms perform much better than such
bounds would predict. Algorithms and theory that provide strong
problem-dependent bounds could help illuminate the key features of what makes a
RL problem hard and reduce the barrier to using RL algorithms in practice. As a
step towards this we derive an algorithm for finite horizon discrete MDPs and
associated analysis that both yields state-of-the art worst-case regret bounds
in the dominant terms and yields substantially tighter bounds if the RL
environment has small environmental norm, which is a function of the variance
of the next-state value functions. An important benefit of our algorithmic is
that it does not require apriori knowledge of a bound on the environmental
norm. As a result of our analysis, we also help address an open learning theory
question~\cite{jiang2018open} about episodic MDPs with a constant upper-bound
on the sum of rewards, providing a regret bound with no $H$-dependence in the
leading term that scales a polynomial function of the number of episodes.",Bug fixes,International Conference on Machine Learning 2019,,http://arxiv.org/abs/1901.00210v4,2019-11-01 20:35:28+00:00,2019-01-01 21:17:21+00:00,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/1901.00210v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00210v4', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.00210v4
Clustering with Distributed Data,"['Soummya Kar', 'Brian Swenson']","We consider $K$-means clustering in networked environments (e.g., internet of
things (IoT) and sensor networks) where data is inherently distributed across
nodes and processing power at each node may be limited. We consider a
clustering algorithm referred to as networked $K$-means, or $NK$-means, which
relies only on local neighborhood information exchange. Information exchange is
limited to low-dimensional statistics and not raw data at the agents. The
proposed approach develops a parametric family of multi-agent clustering
objectives (parameterized by $\rho$) and associated distributed $NK$-means
algorithms (also parameterized by $\rho$). The $NK$-means algorithm with
parameter $\rho$ converges to a set of fixed points relative to the associated
multi-agent objective (designated as `generalized minima'). By appropriate
choice of $\rho$, the set of generalized minima may be brought arbitrarily
close to the set of Lloyd's minima. Thus, the $NK$-means algorithm may be used
to compute Lloyd's minima of the collective dataset up to arbitrary accuracy.",,,,http://arxiv.org/abs/1901.00214v1,2019-01-01 22:01:10+00:00,2019-01-01 22:01:10+00:00,cs.LG,"['cs.LG', 'math.OC', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/1901.00214v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00214v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.00214v1
Opportunistic Learning: Budgeted Cost-Sensitive Learning from Data Streams,"['Mohammad Kachuee', 'Orpaz Goldstein', 'Kimmo Karkkainen', 'Sajad Darabi', 'Majid Sarrafzadeh']","In many real-world learning scenarios, features are only acquirable at a cost
constrained under a budget. In this paper, we propose a novel approach for
cost-sensitive feature acquisition at the prediction-time. The suggested method
acquires features incrementally based on a context-aware feature-value
function. We formulate the problem in the reinforcement learning paradigm, and
introduce a reward function based on the utility of each feature. Specifically,
MC dropout sampling is used to measure expected variations of the model
uncertainty which is used as a feature-value function. Furthermore, we suggest
sharing representations between the class predictor and value function
estimator networks. The suggested approach is completely online and is readily
applicable to stream learning setups. The solution is evaluated on three
different datasets including the well-known MNIST dataset as a benchmark as
well as two cost-sensitive datasets: Yahoo Learning to Rank and a dataset in
the medical domain for diabetes classification. According to the results, the
proposed method is able to efficiently acquire features and make accurate
predictions.",https://openreview.net/forum?id=S1eOHo09KX,"International Conference on Learning Representations (ICLR), 2019",,http://arxiv.org/abs/1901.00243v2,2019-02-18 02:42:39+00:00,2019-01-02 02:33:54+00:00,cs.LG,"['cs.LG', 'cs.AI', 'cs.NE', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/1901.00243v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00243v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.00243v2
Natively Interpretable Machine Learning and Artificial Intelligence: Preliminary Results and Future Directions,"['Christopher J. Hazard', 'Christopher Fusting', 'Michael Resnick', 'Michael Auerbach', 'Michael Meehan', 'Valeri Korobov']","Machine learning models have become more and more complex in order to better
approximate complex functions. Although fruitful in many domains, the added
complexity has come at the cost of model interpretability. The once popular
k-nearest neighbors (kNN) approach, which finds and uses the most similar data
for reasoning, has received much less attention in recent decades due to
numerous problems when compared to other techniques. We show that many of these
historical problems with kNN can be overcome, and our contribution has
applications not only in machine learning but also in online learning, data
synthesis, anomaly detection, model compression, and reinforcement learning,
without sacrificing interpretability. We introduce a synthesis between kNN and
information theory that we hope will provide a clear path towards models that
are innately interpretable and auditable. Through this work we hope to gather
interest in combining kNN with information theory as a promising path to fully
auditable machine learning and artificial intelligence.",16 pages,,,http://arxiv.org/abs/1901.00246v2,2019-01-18 16:35:42+00:00,2019-01-02 03:00:21+00:00,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/1901.00246v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00246v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.00246v2
A Survey on Multi-output Learning,"['Donna Xu', 'Yaxin Shi', 'Ivor W. Tsang', 'Yew-Soon Ong', 'Chen Gong', 'Xiaobo Shen']","Multi-output learning aims to simultaneously predict multiple outputs given
an input. It is an important learning problem due to the pressing need for
sophisticated decision making in real-world applications. Inspired by big data,
the 4Vs characteristics of multi-output imposes a set of challenges to
multi-output learning, in terms of the volume, velocity, variety and veracity
of the outputs. Increasing number of works in the literature have been devoted
to the study of multi-output learning and the development of novel approaches
for addressing the challenges encountered. However, it lacks a comprehensive
overview on different types of challenges of multi-output learning brought by
the characteristics of the multiple outputs and the techniques proposed to
overcome the challenges. This paper thus attempts to fill in this gap to
provide a comprehensive review on this area. We first introduce different
stages of the life cycle of the output labels. Then we present the paradigm on
multi-output learning, including its myriads of output structures, definitions
of its different sub-problems, model evaluation metrics and popular data
repositories used in the study. Subsequently, we review a number of
state-of-the-art multi-output learning methods, which are categorized based on
the challenges.","Paper accepted by IEEE Transactions on Neural Networks and Learning
  Systems",,,http://arxiv.org/abs/1901.00248v2,2019-10-13 10:59:28+00:00,2019-01-02 03:10:24+00:00,cs.LG,"['cs.LG', 'cs.CV', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/1901.00248v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00248v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.00248v2
Learning Humanoid Robot Motions Through Deep Neural Networks,"['Luckeciano Carvalho Melo', 'Marcos Ricardo Omena Albuquerque Maximo', 'Adilson Marques da Cunha']","Controlling a high degrees of freedom humanoid robot is acknowledged as one
of the hardest problems in Robotics. Due to the lack of mathematical models, an
approach frequently employed is to rely on human intuition to design keyframe
movements by hand, usually aided by graphical tools. In this paper, we propose
a learning framework based on neural networks in order to mimic humanoid robot
movements. The developed technique does not make any assumption about the
underlying implementation of the movement, therefore both keyframe and
model-based motions may be learned. The framework was applied in the RoboCup 3D
Soccer Simulation domain and promising results were obtained using the same
network architecture for several motions, even when copying motions from
another teams.",,,,http://arxiv.org/abs/1901.00270v1,2019-01-02 05:46:52+00:00,2019-01-02 05:46:52+00:00,cs.AI,['cs.AI'],"[arxiv.Result.Link('http://arxiv.org/abs/1901.00270v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00270v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.00270v1
Multi-level CNN for lung nodule classification with Gaussian Process assisted hyperparameter optimization,"['Miao Zhang', 'Huiqi Li', 'Juan Lyu', 'Sai Ho Ling', 'Steven Su']","This paper investigates lung nodule classification by using deep neural
networks (DNNs). Hyperparameter optimization in DNNs is a computationally
expensive problem, where evaluating a hyperparameter configuration may take
several hours or even days. Bayesian optimization has been recently introduced
for the automatically searching of optimal hyperparameter configurations of
DNNs. It applies probabilistic surrogate models to approximate the validation
error function of hyperparameter configurations, such as Gaussian processes,
and reduce the computational complexity to a large extent. However, most
existing surrogate models adopt stationary covariance functions to measure the
difference between hyperparameter points based on spatial distance without
considering its spatial locations. This distance-based assumption together with
the condition of constant smoothness throughout the whole hyperparameter search
space clearly violates the property that the points far away from optimal
points usually get similarly poor performance even though each two of them have
huge spatial distance between them. In this paper, a non-stationary kernel is
proposed which allows the surrogate model to adapt to functions whose
smoothness varies with the spatial location of inputs, and a multi-level
convolutional neural network (ML-CNN) is built for lung nodule classification
whose hyperparameter configuration is optimized by using the proposed
non-stationary kernel based Gaussian surrogate model. Our algorithm searches
the surrogate for optimal setting via hyperparameter importance based
evolutionary strategy, and the experiments demonstrate our algorithm
outperforms manual tuning and well-established hyperparameter optimization
methods such as Random search, Gaussian processes with stationary kernels, and
recently proposed Hyperparameter Optimization via RBF and Dynamic coordinate
search.",,,,http://arxiv.org/abs/1901.00276v1,2019-01-02 06:20:04+00:00,2019-01-02 06:20:04+00:00,cs.LG,"['cs.LG', 'eess.IV']","[arxiv.Result.Link('http://arxiv.org/abs/1901.00276v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00276v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.00276v1
Elimination of All Bad Local Minima in Deep Learning,"['Kenji Kawaguchi', 'Leslie Pack Kaelbling']","In this paper, we theoretically prove that adding one special neuron per
output unit eliminates all suboptimal local minima of any deep neural network,
for multi-class classification, binary classification, and regression with an
arbitrary loss function, under practical assumptions. At every local minimum of
any deep neural network with these added neurons, the set of parameters of the
original neural network (without added neurons) is guaranteed to be a global
minimum of the original neural network. The effects of the added neurons are
proven to automatically vanish at every local minimum. Moreover, we provide a
novel theoretical characterization of a failure mode of eliminating suboptimal
local minima via an additional theorem and several examples. This paper also
introduces a novel proof technique based on the perturbable gradient basis
(PGB) necessary condition of local minima, which provides new insight into the
elimination of local minima and is applicable to analyze various models and
transformations of objective functions beyond the elimination of local minima.",Accepted to appear in AISTATS 2020,,,http://arxiv.org/abs/1901.00279v2,2020-01-15 22:20:50+00:00,2019-01-02 06:40:36+00:00,cs.LG,"['cs.LG', 'cs.NE', 'math.OC', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/1901.00279v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00279v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.00279v2
Ethically Aligned Opportunistic Scheduling for Productive Laziness,"['Han Yu', 'Chunyan Miao', 'Yongqing Zheng', 'Lizhen Cui', 'Simon Fauvel', 'Cyril Leung']","In artificial intelligence (AI) mediated workforce management systems (e.g.,
crowdsourcing), long-term success depends on workers accomplishing tasks
productively and resting well. This dual objective can be summarized by the
concept of productive laziness. Existing scheduling approaches mostly focus on
efficiency but overlook worker wellbeing through proper rest. In order to
enable workforce management systems to follow the IEEE Ethically Aligned Design
guidelines to prioritize worker wellbeing, we propose a distributed
Computational Productive Laziness (CPL) approach in this paper. It
intelligently recommends personalized work-rest schedules based on local data
concerning a worker's capabilities and situational factors to incorporate
opportunistic resting and achieve superlinear collective productivity without
the need for explicit coordination messages. Extensive experiments based on a
real-world dataset of over 5,000 workers demonstrate that CPL enables workers
to spend 70% of the effort to complete 90% of the tasks on average, providing
more ethically aligned scheduling than existing approaches.",,"Proceedings of the 2nd AAAI/ACM Conference on Artificial
  Intelligence, Ethics, and Society (AIES-19), 2019",,http://arxiv.org/abs/1901.00298v1,2019-01-02 09:01:07+00:00,2019-01-02 09:01:07+00:00,cs.AI,['cs.AI'],"[arxiv.Result.Link('http://arxiv.org/abs/1901.00298v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00298v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.00298v1
Warm-starting Contextual Bandits: Robustly Combining Supervised and Bandit Feedback,"['Chicheng Zhang', 'Alekh Agarwal', 'Hal Daumé III', 'John Langford', 'Sahand N Negahban']","We investigate the feasibility of learning from a mix of both fully-labeled
supervised data and contextual bandit data. We specifically consider settings
in which the underlying learning signal may be different between these two data
sources. Theoretically, we state and prove no-regret algorithms for learning
that is robust to misaligned cost distributions between the two sources.
Empirically, we evaluate some of these algorithms on a large selection of
datasets, showing that our approach is both feasible and helpful in practice.","42 pages, 21 figures, ICML 2019",,,http://arxiv.org/abs/1901.00301v2,2019-06-21 19:25:56+00:00,2019-01-02 09:15:02+00:00,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/1901.00301v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00301v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.00301v2
"KI, Philosophie, Logik",['Karl Schlechta'],"This is a short (and personal) introduction in German to the connections
between artificial intelligence, philosophy, and logic, and to the author's
work.
  Dies ist eine kurze (und persoenliche) Einfuehrung in die Zusammenhaenge
zwischen Kuenstlicher Intelligenz, Philosophie, und Logik, und in die Arbeiten
des Autors.",in German,,,http://arxiv.org/abs/1901.00365v1,2018-12-27 10:29:47+00:00,2018-12-27 10:29:47+00:00,cs.AI,['cs.AI'],"[arxiv.Result.Link('http://arxiv.org/abs/1901.00365v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00365v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.00365v1
Optimizing Bit-Serial Matrix Multiplication for Reconfigurable Computing,"['Yaman Umuroglu', 'Davide Conficconi', 'Lahiru Rasnayake', 'Thomas B. Preusser', 'Magnus Sjalander']","Matrix-matrix multiplication is a key computational kernel for numerous
applications in science and engineering, with ample parallelism and data
locality that lends itself well to high-performance implementations. Many
matrix multiplication-dependent applications can use reduced-precision integer
or fixed-point representations to increase their performance and energy
efficiency while still offering adequate quality of results. However, precision
requirements may vary between different application phases or depend on input
data, rendering constant-precision solutions ineffective. BISMO, a vectorized
bit-serial matrix multiplication overlay for reconfigurable computing,
previously utilized the excellent binary-operation performance of FPGAs to
offer a matrix multiplication performance that scales with required precision
and parallelism. We show how BISMO can be scaled up on Xilinx FPGAs using an
arithmetic architecture that better utilizes 6-LUTs. The improved BISMO
achieves a peak performance of 15.4 binary TOPS on the Ultra96 board with a
Xilinx UltraScale+ MPSoC.","Invited paper at ACM TRETS as extension of FPL'18 paper
  arXiv:1806.08862",,10.1145/3337929,http://arxiv.org/abs/1901.00370v2,2019-06-11 11:12:17+00:00,2019-01-02 14:05:48+00:00,cs.AR,['cs.AR'],"[arxiv.Result.Link('http://dx.doi.org/10.1145/3337929', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1901.00370v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00370v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.00370v2
A Full Probabilistic Model for Yes/No Type Crowdsourcing in Multi-Class Classification,"['Belen Saldias', 'Pavlos Protopapas', 'Karim Pichara']","Crowdsourcing has become widely used in supervised scenarios where training
sets are scarce and difficult to obtain. Most crowdsourcing models in the
literature assume labelers can provide answers to full questions. In
classification contexts, full questions require a labeler to discern among all
possible classes. Unfortunately, discernment is not always easy in realistic
scenarios. Labelers may not be experts in differentiating all classes. In this
work, we provide a full probabilistic model for a shorter type of queries. Our
shorter queries only require ""yes"" or ""no"" responses. Our model estimates a
joint posterior distribution of matrices related to labelers' confusions and
the posterior probability of the class of every object. We developed an
approximate inference approach, using Monte Carlo Sampling and Black Box
Variational Inference, which provides the derivation of the necessary
gradients. We built two realistic crowdsourcing scenarios to test our model.
The first scenario queries for irregular astronomical time-series. The second
scenario relies on the image classification of animals. We achieved results
that are comparable with those of full query crowdsourcing. Furthermore, we
show that modeling labelers' failures plays an important role in estimating
true classes. Finally, we provide the community with two real datasets obtained
from our crowdsourcing experiments. All our code is publicly available.","SIAM International Conference on Data Mining (SDM19), 9 official
  pages, 5 supplementary pages","Proceedings of the 2019 SIAM International Conference on Data
  Mining",10.1137/1.9781611975673.85,http://arxiv.org/abs/1901.00397v3,2019-08-14 02:57:47+00:00,2019-01-02 14:40:10+00:00,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://dx.doi.org/10.1137/1.9781611975673.85', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1901.00397v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00397v3', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.00397v3
The capacity of feedforward neural networks,"['Pierre Baldi', 'Roman Vershynin']","A long standing open problem in the theory of neural networks is the
development of quantitative methods to estimate and compare the capabilities of
different architectures. Here we define the capacity of an architecture by the
binary logarithm of the number of functions it can compute, as the synaptic
weights are varied. The capacity provides an upper bound on the number of bits
that can be extracted from the training data and stored in the architecture
during learning. We study the capacity of layered, fully-connected,
architectures of linear threshold neurons with $L$ layers of size $n_1,n_2,
\ldots, n_L$ and show that in essence the capacity is given by a cubic
polynomial in the layer sizes: $C(n_1,\ldots, n_L)=\sum_{k=1}^{L-1}
\min(n_1,\ldots,n_k)n_kn_{k+1}$, where layers that are smaller than all
previous layers act as bottlenecks. In proving the main result, we also develop
new techniques (multiplexing, enrichment, and stacking) as well as new bounds
on the capacity of finite sets. We use the main result to identify
architectures with maximal or minimal capacity under a number of natural
constraints. This leads to the notion of structural regularization for deep
architectures. While in general, everything else being equal, shallow networks
compute more functions than deep networks, the functions computed by deep
networks are more regular and ""interesting"".",49 pages. Introduction is expanded and conclusion is added,,,http://arxiv.org/abs/1901.00434v2,2019-03-27 21:06:06+00:00,2019-01-02 16:05:28+00:00,cs.LG,"['cs.LG', 'cs.NE', 'math.CO', 'stat.ML', '68Q32, 06E30, 92B20']","[arxiv.Result.Link('http://arxiv.org/abs/1901.00434v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00434v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.00434v2
BMF: Block matrix approach to factorization of large scale data,"['Prasad G Bhavana', 'Vineet C Nair']","Matrix Factorization (MF) on large scale matrices is computationally as well
as memory intensive task. Alternative convergence techniques are needed when
the size of the input matrix is higher than the available memory on a Central
Processing Unit (CPU) and Graphical Processing Unit (GPU). While alternating
least squares (ALS) convergence on CPU could take forever, loading all the
required matrices on to GPU memory may not be possible when the dimensions are
significantly higher. Hence we introduce a novel technique that is based on
considering the entire data into a block matrix and relies on factorization at
a block level.",Disagreement on success criteria of the method with my guide,,,http://arxiv.org/abs/1901.00444v2,2019-01-18 16:46:17+00:00,2019-01-02 16:25:54+00:00,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/1901.00444v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00444v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.00444v2
SGD Converges to Global Minimum in Deep Learning via Star-convex Path,"['Yi Zhou', 'Junjie Yang', 'Huishuai Zhang', 'Yingbin Liang', 'Vahid Tarokh']","Stochastic gradient descent (SGD) has been found to be surprisingly effective
in training a variety of deep neural networks. However, there is still a lack
of understanding on how and why SGD can train these complex networks towards a
global minimum. In this study, we establish the convergence of SGD to a global
minimum for nonconvex optimization problems that are commonly encountered in
neural network training. Our argument exploits the following two important
properties: 1) the training loss can achieve zero value (approximately), which
has been widely observed in deep learning; 2) SGD follows a star-convex path,
which is verified by various experiments in this paper. In such a context, our
analysis shows that SGD, although has long been considered as a randomized
algorithm, converges in an intrinsically deterministic manner to a global
minimum.",ICLR2019,,,http://arxiv.org/abs/1901.00451v1,2019-01-02 17:09:45+00:00,2019-01-02 17:09:45+00:00,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/1901.00451v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00451v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.00451v1
A CNN adapted to time series for the classification of Supernovae,"['Anthony Brunel', 'Johanna Pasquet', 'Jérôme Pasquet', 'Nancy Rodriguez', 'Frédéric Comby', 'Dominique Fouchez', 'Marc Chaumont']","Cosmologists are facing the problem of the analysis of a huge quantity of
data when observing the sky. The methods used in cosmology are, for the most of
them, relying on astrophysical models, and thus, for the classification, they
usually use a machine learning approach in two-steps, which consists in, first,
extracting features, and second, using a classifier. In this paper, we are
specifically studying the supernovae phenomenon and especially the binary
classification ""I.a supernovae versus not-I.a supernovae"". We present two
Convolutional Neural Networks (CNNs) defeating the current state-of-the-art.
The first one is adapted to time series and thus to the treatment of supernovae
light-curves. The second one is based on a Siamese CNN and is suited to the
nature of data, i.e. their sparsity and their weak quantity (small learning
database).","IS&T International Symposium on Electronic Imaging, EI'2019, Color
  Imaging XXIV: Displaying, Processing, Hardcopy, and Applications, Burlingame
  (suburb of San Francisco), California USA, 13 - 17 January, 2019, 8 pages.
  The CNN is downloadable there:
  https://github.com/Anzzy30/SupernovaeClassification",,,http://arxiv.org/abs/1901.00461v1,2019-01-02 17:49:06+00:00,2019-01-02 17:49:06+00:00,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/1901.00461v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00461v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.00461v1
Computational model discovery with reinforcement learning,"['Maxime Bassenne', 'Adrián Lozano-Durán']","The motivation of this study is to leverage recent breakthroughs in
artificial intelligence research to unlock novel solutions to important
scientific problems encountered in computational science. To address the human
intelligence limitations in discovering reduced-order models, we propose to
supplement human thinking with artificial intelligence. Our three-pronged
strategy consists of learning (i) models expressed in analytical form, (ii)
which are evaluated a posteriori, and iii) using exclusively integral
quantities from the reference solution as prior knowledge. In point (i), we
pursue interpretable models expressed symbolically as opposed to black-box
neural networks, the latter only being used during learning to efficiently
parameterize the large search space of possible models. In point (ii), learned
models are dynamically evaluated a posteriori in the computational solver
instead of based on a priori information from preprocessed high-fidelity data,
thereby accounting for the specificity of the solver at hand such as its
numerics. Finally in point (iii), the exploration of new models is solely
guided by predefined integral quantities, e.g., averaged quantities of
engineering interest in Reynolds-averaged or large-eddy simulations (LES). We
use a coupled deep reinforcement learning framework and computational solver to
concurrently achieve these objectives. The combination of reinforcement
learning with objectives (i), (ii) and (iii) differentiate our work from
previous modeling attempts based on machine learning. In this report, we
provide a high-level description of the model discovery framework with
reinforcement learning. The method is detailed for the application of
discovering missing terms in differential equations. An elementary
instantiation of the method is described that discovers missing terms in the
Burgers' equation.",,,,http://arxiv.org/abs/2001.00008v1,2019-12-29 22:56:40+00:00,2019-12-29 22:56:40+00:00,cs.LG,"['cs.LG', 'physics.flu-dyn', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2001.00008v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00008v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00008v1
Differentially Private M-band Wavelet-Based Mechanisms in Machine Learning Environments,"['Kenneth Choi', 'Tony Lee']","In the post-industrial world, data science and analytics have gained
paramount importance regarding digital data privacy. Improper methods of
establishing privacy for accessible datasets can compromise large amounts of
user data even if the adversary has a small amount of preliminary knowledge of
a user. Many researchers have been developing high-level privacy-preserving
mechanisms that also retain the statistical integrity of the data to apply to
machine learning. Recent developments of differential privacy, such as the
Laplace and Privelet mechanisms, drastically decrease the probability that an
adversary can distinguish the elements in a data set and thus extract user
information. In this paper, we develop three privacy-preserving mechanisms with
the discrete M-band wavelet transform that embed noise into data. The first two
methods (LS and LS+) add noise through a Laplace-Sigmoid distribution that
multiplies Laplace-distributed values with the sigmoid function, and the third
method utilizes pseudo-quantum steganography to embed noise into the data. We
then show that our mechanisms successfully retain both differential privacy and
learnability through statistical analysis in various machine learning
environments.","Part-Time Research Assistant/Helper: Tony Lee; 49 pages, 20 figures,
  1 table, to be published by International Press of Boston",,,http://arxiv.org/abs/2001.00012v2,2020-02-18 20:28:25+00:00,2019-12-30 18:07:37+00:00,cs.LG,"['cs.LG', 'cs.CR', 'stat.ML', '68T01 (Primary) 94A60 (Secondary)']","[arxiv.Result.Link('http://arxiv.org/abs/2001.00012v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00012v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00012v2
On the Resilience of Deep Learning for Reduced-voltage FPGAs,"['Kamyar Givaki', 'Behzad Salami', 'Reza Hojabr', 'S. M. Reza Tayaranian', 'Ahmad Khonsari', 'Dara Rahmati', 'Saeid Gorgin', 'Adrian Cristal', 'Osman S. Unsal']","Deep Neural Networks (DNNs) are inherently computation-intensive and also
power-hungry. Hardware accelerators such as Field Programmable Gate Arrays
(FPGAs) are a promising solution that can satisfy these requirements for both
embedded and High-Performance Computing (HPC) systems. In FPGAs, as well as
CPUs and GPUs, aggressive voltage scaling below the nominal level is an
effective technique for power dissipation minimization. Unfortunately, bit-flip
faults start to appear as the voltage is scaled down closer to the transistor
threshold due to timing issues, thus creating a resilience issue.
  This paper experimentally evaluates the resilience of the training phase of
DNNs in the presence of voltage underscaling related faults of FPGAs,
especially in on-chip memories. Toward this goal, we have experimentally
evaluated the resilience of LeNet-5 and also a specially designed network for
CIFAR-10 dataset with different activation functions of Rectified Linear Unit
(Relu) and Hyperbolic Tangent (Tanh). We have found that modern FPGAs are
robust enough in extremely low-voltage levels and that low-voltage related
faults can be automatically masked within the training iterations, so there is
no need for costly software- or hardware-oriented fault mitigation techniques
like ECC. Approximately 10% more training iterations are needed to fill the gap
in the accuracy. This observation is the result of the relatively low rate of
undervolting faults, i.e., <0.1\%, measured on real FPGA fabrics. We have also
increased the fault rate significantly for the LeNet-5 network by randomly
generated fault injection campaigns and observed that the training accuracy
starts to degrade. When the fault rate increases, the network with Tanh
activation function outperforms the one with Relu in terms of accuracy, e.g.,
when the fault rate is 30% the accuracy difference is 4.92%.",,,,http://arxiv.org/abs/2001.00053v1,2019-12-26 15:08:22+00:00,2019-12-26 15:08:22+00:00,cs.LG,"['cs.LG', 'cs.NE']","[arxiv.Result.Link('http://arxiv.org/abs/2001.00053v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00053v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00053v1
Deep Learning Training with Simulated Approximate Multipliers,"['Issam Hammad', 'Kamal El-Sankary', 'Jason Gu']","This paper presents by simulation how approximate multipliers can be utilized
to enhance the training performance of convolutional neural networks (CNNs).
Approximate multipliers have significantly better performance in terms of
speed, power, and area compared to exact multipliers. However, approximate
multipliers have an inaccuracy which is defined in terms of the Mean Relative
Error (MRE). To assess the applicability of approximate multipliers in
enhancing CNN training performance, a simulation for the impact of approximate
multipliers error on CNN training is presented. The paper demonstrates that
using approximate multipliers for CNN training can significantly enhance the
performance in terms of speed, power, and area at the cost of a small negative
impact on the achieved accuracy. Additionally, the paper proposes a hybrid
training method which mitigates this negative impact on the accuracy. Using the
proposed hybrid method, the training can start using approximate multipliers
then switches to exact multipliers for the last few epochs. Using this method,
the performance benefits of approximate multipliers in terms of speed, power,
and area can be attained for a large portion of the training stage. On the
other hand, the negative impact on the accuracy is diminished by using the
exact multipliers for the last epochs of training.","Presented at: IEEE International Conference on Robotics and
  Biomimetics (ROBIO) 2019, Dali, China, December 2019. WINNER OF THE MOZI BEST
  PAPER IN AI AWARD","2019 IEEE International Conference on Robotics and Biomimetics
  (ROBIO)",10.1109/ROBIO49542.2019.8961780,http://arxiv.org/abs/2001.00060v2,2020-04-18 13:22:32+00:00,2019-12-26 12:50:06+00:00,cs.LG,"['cs.LG', 'cs.CV', 'cs.PF', 'eess.IV', 'stat.ML']","[arxiv.Result.Link('http://dx.doi.org/10.1109/ROBIO49542.2019.8961780', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2001.00060v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00060v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00060v2
Visual Evaluation of Generative Adversarial Networks for Time Series Data,"['Hiba Arnout', 'Johannes Kehrer', 'Johanna Bronner', 'Thomas Runkler']","A crucial factor to trust Machine Learning (ML) algorithm decisions is a good
representation of its application field by the training dataset. This is
particularly true when parts of the training data have been artificially
generated to overcome common training problems such as lack of data or
imbalanced dataset. Over the last few years, Generative Adversarial Networks
(GANs) have shown remarkable results in generating realistic data. However,
this ML approach lacks an objective function to evaluate the quality of the
generated data. Numerous GAN applications focus on generating image data mostly
because they can be easily evaluated by a human eye. Less efforts have been
made to generate time series data. Assessing their quality is more complicated,
particularly for technical data. In this paper, we propose a human-centered
approach supporting a ML or domain expert to accomplish this task using Visual
Analytics (VA) techniques. The presented approach consists of two views, namely
a GAN Iteration View showing similarity metrics between real and generated data
over the iterations of the generation process and a Detailed Comparative View
equipped with different time series visualizations such as TimeHistograms, to
compare the generated data at different iteration steps. Starting from the GAN
Iteration View, the user can choose suitable iteration steps for detailed
inspection. We evaluate our approach with a usage scenario that enabled an
efficient comparison of two different GAN models.","To appear in the Proceedings of the Human-Centered AI:
  Trustworthiness of AI Models & Data (HAI) track at AAAI Fall Symposium, DC,
  November 7-9, 2019",,,http://arxiv.org/abs/2001.00062v1,2019-12-23 13:59:33+00:00,2019-12-23 13:59:33+00:00,cs.LG,"['cs.LG', 'cs.HC', 'eess.IV']","[arxiv.Result.Link('http://arxiv.org/abs/2001.00062v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00062v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00062v1
privGAN: Protecting GANs from membership inference attacks at low cost,"['Sumit Mukherjee', 'Yixi Xu', 'Anusua Trivedi', 'Juan Lavista Ferres']","Generative Adversarial Networks (GANs) have made releasing of synthetic
images a viable approach to share data without releasing the original dataset.
It has been shown that such synthetic data can be used for a variety of
downstream tasks such as training classifiers that would otherwise require the
original dataset to be shared. However, recent work has shown that the GAN
models and their synthetically generated data can be used to infer the training
set membership by an adversary who has access to the entire dataset and some
auxiliary information. Current approaches to mitigate this problem (such as
DPGAN) lead to dramatically poorer generated sample quality than the original
non--private GANs. Here we develop a new GAN architecture (privGAN), where the
generator is trained not only to cheat the discriminator but also to defend
membership inference attacks. The new mechanism provides protection against
this mode of attack while leading to negligible loss in downstream
performances. In addition, our algorithm has been shown to explicitly prevent
overfitting to the training set, which explains why our protection is so
effective. The main contributions of this paper are: i) we propose a novel GAN
architecture that can generate synthetic data in a privacy preserving manner
without additional hyperparameter tuning and architecture selection, ii) we
provide a theoretical understanding of the optimal solution of the privGAN loss
function, iii) we demonstrate the effectiveness of our model against several
white and black--box attacks on several benchmark datasets, iv) we demonstrate
on three common benchmark datasets that synthetic images generated by privGAN
lead to negligible loss in downstream performance when compared against
non--private GANs.",,,,http://arxiv.org/abs/2001.00071v4,2020-12-13 18:27:26+00:00,2019-12-31 20:47:21+00:00,cs.LG,"['cs.LG', 'cs.CR', 'cs.CV', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2001.00071v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00071v4', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00071v4
Scalable Hierarchical Clustering with Tree Grafting,"['Nicholas Monath', 'Ari Kobren', 'Akshay Krishnamurthy', 'Michael Glass', 'Andrew McCallum']","We introduce Grinch, a new algorithm for large-scale, non-greedy hierarchical
clustering with general linkage functions that compute arbitrary similarity
between two point sets. The key components of Grinch are its rotate and graft
subroutines that efficiently reconfigure the hierarchy as new points arrive,
supporting discovery of clusters with complex structure. Grinch is motivated by
a new notion of separability for clustering with linkage functions: we prove
that when the model is consistent with a ground-truth clustering, Grinch is
guaranteed to produce a cluster tree containing the ground-truth, independent
of data arrival order. Our empirical results on benchmark and author
coreference datasets (with standard and learned linkage functions) show that
Grinch is more accurate than other scalable methods, and orders of magnitude
faster than hierarchical agglomerative clustering.","23 pages (appendix included), published at KDD 2019",,10.1145/3292500.3330929,http://arxiv.org/abs/2001.00076v1,2019-12-31 20:56:15+00:00,2019-12-31 20:56:15+00:00,cs.LG,"['cs.LG', 'cs.DS', 'stat.ML']","[arxiv.Result.Link('http://dx.doi.org/10.1145/3292500.3330929', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2001.00076v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00076v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00076v1
Avoiding Spurious Local Minima in Deep Quadratic Networks,"['Abbas Kazemipour', 'Brett W. Larsen', 'Shaul Druckmann']","Despite their practical success, a theoretical understanding of the loss
landscape of neural networks has proven challenging due to the
high-dimensional, non-convex, and highly nonlinear structure of such models. In
this paper, we characterize the training landscape of the mean squared error
loss for neural networks with quadratic activation functions. We prove
existence of spurious local minima and saddle points which can be escaped
easily with probability one when the number of neurons is greater than or equal
to the input dimension and the norm of the training samples is used as a
regressor. We prove that deep overparameterized neural networks with quadratic
activations benefit from similar nice landscape properties. Our theoretical
results are independent of data distribution and fill the existing gap in
theory for two-layer quadratic neural networks. Finally, we empirically
demonstrate convergence to a global minimum for these problems.","36 pages; added deep network experiments, results for population loss",,,http://arxiv.org/abs/2001.00098v2,2020-07-20 00:40:15+00:00,2019-12-31 22:31:11+00:00,cs.LG,"['cs.LG', 'math.OC', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2001.00098v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00098v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00098v2
PAC Confidence Sets for Deep Neural Networks via Calibrated Prediction,"['Sangdon Park', 'Osbert Bastani', 'Nikolai Matni', 'Insup Lee']","We propose an algorithm combining calibrated prediction and generalization
bounds from learning theory to construct confidence sets for deep neural
networks with PAC guarantees---i.e., the confidence set for a given input
contains the true label with high probability. We demonstrate how our approach
can be used to construct PAC confidence sets on ResNet for ImageNet, a visual
object tracking model, and a dynamics model for the half-cheetah reinforcement
learning problem.",Accepted to ICLR 2020,,,http://arxiv.org/abs/2001.00106v2,2020-02-15 19:50:39+00:00,2019-12-31 23:02:01+00:00,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2001.00106v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00106v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00106v2
Long-Term Visitation Value for Deep Exploration in Sparse Reward Reinforcement Learning,"['Simone Parisi', 'Davide Tateo', 'Maximilian Hensel', ""Carlo D'Eramo"", 'Jan Peters', 'Joni Pajarinen']","Reinforcement learning with sparse rewards is still an open challenge.
Classic methods rely on getting feedback via extrinsic rewards to train the
agent, and in situations where this occurs very rarely the agent learns slowly
or cannot learn at all. Similarly, if the agent receives also rewards that
create suboptimal modes of the objective function, it will likely prematurely
stop exploring. More recent methods add auxiliary intrinsic rewards to
encourage exploration. However, auxiliary rewards lead to a non-stationary
target for the Q-function. In this paper, we present a novel approach that (1)
plans exploration actions far into the future by using a long-term visitation
count, and (2) decouples exploration and exploitation by learning a separate
function assessing the exploration value of the actions. Contrary to existing
methods which use models of reward and dynamics, our approach is off-policy and
model-free. We further propose new tabular environments for benchmarking
exploration in reinforcement learning. Empirical results on classic and novel
benchmarks show that the proposed approach outperforms existing methods in
environments with sparse rewards, especially in the presence of rewards that
create suboptimal modes of the objective function. Results also suggest that
our approach scales gracefully with the size of the environment. Source code is
available at https://github.com/sparisi/visit-value-explore",,"Algorithms 2022, 15(3), 81",10.3390/a15030081,http://arxiv.org/abs/2001.00119v2,2022-03-03 06:51:10+00:00,2020-01-01 01:01:15+00:00,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://dx.doi.org/10.3390/a15030081', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2001.00119v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00119v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00119v2
Reinforcement Learning with Goal-Distance Gradient,"['Kai Jiang', 'XiaoLong Qin']","Reinforcement learning usually uses the feedback rewards of environmental to
train agents. But the rewards in the actual environment are sparse, and even
some environments will not rewards. Most of the current methods are difficult
to get good performance in sparse reward or non-reward environments. Although
using shaped rewards is effective when solving sparse reward tasks, it is
limited to specific problems and learning is also susceptible to local optima.
We propose a model-free method that does not rely on environmental rewards to
solve the problem of sparse rewards in the general environment. Our method use
the minimum number of transitions between states as the distance to replace the
rewards of environmental, and proposes a goal-distance gradient to achieve
policy improvement. We also introduce a bridge point planning method based on
the characteristics of our method to improve exploration efficiency, thereby
solving more complex tasks. Experiments show that our method performs better on
sparse reward and local optimal problems in complex environments than previous
work.",,,,http://arxiv.org/abs/2001.00127v2,2020-01-10 12:26:33+00:00,2020-01-01 02:37:34+00:00,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2001.00127v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00127v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00127v2
PatDNN: Achieving Real-Time DNN Execution on Mobile Devices with Pattern-based Weight Pruning,"['Wei Niu', 'Xiaolong Ma', 'Sheng Lin', 'Shihao Wang', 'Xuehai Qian', 'Xue Lin', 'Yanzhi Wang', 'Bin Ren']","With the emergence of a spectrum of high-end mobile devices, many
applications that formerly required desktop-level computation capability are
being transferred to these devices. However, executing the inference of Deep
Neural Networks (DNNs) is still challenging considering high computation and
storage demands, specifically, if real-time performance with high accuracy is
needed. Weight pruning of DNNs is proposed, but existing schemes represent two
extremes in the design space: non-structured pruning is fine-grained, accurate,
but not hardware friendly; structured pruning is coarse-grained,
hardware-efficient, but with higher accuracy loss. In this paper, we introduce
a new dimension, fine-grained pruning patterns inside the coarse-grained
structures, revealing a previously unknown point in design space. With the
higher accuracy enabled by fine-grained pruning patterns, the unique insight is
to use the compiler to re-gain and guarantee high hardware efficiency. In other
words, our method achieves the best of both worlds, and is desirable across
theory/algorithm, compiler, and hardware levels. The proposed PatDNN is an
end-to-end framework to efficiently execute DNN on mobile devices with the help
of a novel model compression technique (pattern-based pruning based on extended
ADMM solution framework) and a set of thorough architecture-aware compiler- and
code generation-based optimizations (filter kernel reordering, compressed
weight storage, register load redundancy elimination, and parameter
auto-tuning). Evaluation results demonstrate that PatDNN outperforms three
state-of-the-art end-to-end DNN frameworks, TensorFlow Lite, TVM, and Alibaba
Mobile Neural Network with speedup up to 44.5x, 11.4x, and 7.1x, respectively,
with no accuracy compromise. Real-time inference of representative large-scale
DNNs (e.g., VGG-16, ResNet-50) can be achieved using mobile devices.","To be published in the Proceedings of Twenty-Fifth International
  Conference on Architectural Support for Programming Languages and Operating
  Systems (ASPLOS 20)",,10.1145/3373376.3378534,http://arxiv.org/abs/2001.00138v4,2020-01-22 04:13:06+00:00,2020-01-01 04:52:07+00:00,cs.LG,"['cs.LG', 'cs.CV', 'cs.DC']","[arxiv.Result.Link('http://dx.doi.org/10.1145/3373376.3378534', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2001.00138v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00138v4', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00138v4
Dual Adversarial Domain Adaptation,"['Yuntao Du', 'Zhiwen Tan', 'Qian Chen', 'Xiaowen Zhang', 'Yirong Yao', 'Chongjun Wang']","Unsupervised domain adaptation aims at transferring knowledge from the
labeled source domain to the unlabeled target domain. Previous adversarial
domain adaptation methods mostly adopt the discriminator with binary or
$K$-dimensional output to perform marginal or conditional alignment
independently. Recent experiments have shown that when the discriminator is
provided with domain information in both domains and label information in the
source domain, it is able to preserve the complex multimodal information and
high semantic information in both domains. Following this idea, we adopt a
discriminator with $2K$-dimensional output to perform both domain-level and
class-level alignments simultaneously in a single discriminator. However, a
single discriminator can not capture all the useful information across domains
and the relationships between the examples and the decision boundary are rarely
explored before. Inspired by multi-view learning and latest advances in domain
adaptation, besides the adversarial process between the discriminator and the
feature extractor, we also design a novel mechanism to make two discriminators
pit against each other, so that they can provide diverse information for each
other and avoid generating target features outside the support of the source
domain. To the best of our knowledge, it is the first time to explore a dual
adversarial strategy in domain adaptation. Moreover, we also use the
semi-supervised learning regularization to make the representations more
discriminative. Comprehensive experiments on two real-world datasets verify
that our method outperforms several state-of-the-art domain adaptation methods.",,,,http://arxiv.org/abs/2001.00153v1,2020-01-01 07:10:09+00:00,2020-01-01 07:10:09+00:00,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2001.00153v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00153v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00153v1
Ensemble emotion recognizing with multiple modal physiological signals,"['Jing Zhang', 'Yong Zhang', 'Suhua Zhan', 'Cheng Cheng']","Physiological signals that provide the objective repression of human
affective states are attracted increasing attention in the emotion recognition
field. However, the single signal is difficult to obtain completely and
accurately description for emotion. Multiple physiological signals fusing
models, building the uniform classification model by means of consistent and
complementary information from different emotions to improve recognition
performance. Original fusing models usually choose the particular
classification method to recognition, which is ignoring different distribution
of multiple signals. Aiming above problems, in this work, we propose an emotion
classification model through multiple modal physiological signals for different
emotions. Features are extracted from EEG, EMG, EOG signals for characterizing
emotional state on valence and arousal levels. For characterization, four bands
filtering theta, beta, alpha, gamma for signal preprocessing are adopted and
three Hjorth parameters are computing as features. To improve classification
performance, an ensemble classifier is built. Experiments are conducted on the
benchmark DEAP datasets. For the two-class task, the best result on arousal is
94.42\%, the best result on valence is 94.02\%, respectively. For the
four-class task, the highest average classification accuracy is 90.74, and it
shows good stability. The influence of different peripheral physiological
signals for results is also analyzed in this paper.",under review for Multimedia tools and applications,,,http://arxiv.org/abs/2001.00191v1,2020-01-01 11:44:43+00:00,2020-01-01 11:44:43+00:00,cs.LG,"['cs.LG', 'eess.SP', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2001.00191v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00191v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00191v1
Histogram Layers for Texture Analysis,"['Joshua Peeples', 'Weihuang Xu', 'Alina Zare']","An essential aspect of texture analysis is the extraction of features that
describe the distribution of values in local, spatial regions. We present a
localized histogram layer for artificial neural networks. Instead of computing
global histograms as done previously, the proposed histogram layer directly
computes the local, spatial distribution of features for texture analysis and
parameters for the layer are estimated during backpropagation. We compare our
method with state-of-the-art texture encoding methods such as the Deep Encoding
Network Pooling, Deep Texture Encoding Network, Fisher Vector convolutional
neural network, and Multi-level Texture Encoding and Representation on three
material/texture datasets: (1) the Describable Texture Dataset; (2) an
extension of the ground terrain in outdoor scenes; (3) and a subset of the
Materials in Context dataset. Results indicate that the inclusion of the
proposed histogram layer improves performance. The source code for the
histogram layer is publicly available:
https://github.com/GatorSense/Histogram_Layer.","13 pages, 8 figures; Accepted to IEEE Transactions on Artificial
  Intelligence",,10.1109/TAI.2021.3135804,http://arxiv.org/abs/2001.00215v12,2021-12-28 17:15:01+00:00,2020-01-01 14:41:54+00:00,cs.LG,"['cs.LG', 'cs.CV', 'stat.ML']","[arxiv.Result.Link('http://dx.doi.org/10.1109/TAI.2021.3135804', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2001.00215v12', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00215v12', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00215v12
Lossless Compression of Deep Neural Networks,"['Thiago Serra', 'Abhinav Kumar', 'Srikumar Ramalingam']","Deep neural networks have been successful in many predictive modeling tasks,
such as image and language recognition, where large neural networks are often
used to obtain good accuracy. Consequently, it is challenging to deploy these
networks under limited computational resources, such as in mobile devices. In
this work, we introduce an algorithm that removes units and layers of a neural
network while not changing the output that is produced, which thus implies a
lossless compression. This algorithm, which we denote as LEO (Lossless
Expressiveness Optimization), relies on Mixed-Integer Linear Programming (MILP)
to identify Rectified Linear Units (ReLUs) with linear behavior over the input
domain. By using L1 regularization to induce such behavior, we can benefit from
training over a larger architecture than we would later use in the environment
where the trained neural network is deployed.",CPAIOR 2020 (to appear),,,http://arxiv.org/abs/2001.00218v3,2020-02-22 16:09:43+00:00,2020-01-01 15:04:43+00:00,cs.LG,"['cs.LG', 'cs.DS', 'math.OC', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2001.00218v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00218v3', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00218v3
Meta Reinforcement Learning with Autonomous Inference of Subtask Dependencies,"['Sungryull Sohn', 'Hyunjae Woo', 'Jongwook Choi', 'Honglak Lee']","We propose and address a novel few-shot RL problem, where a task is
characterized by a subtask graph which describes a set of subtasks and their
dependencies that are unknown to the agent. The agent needs to quickly adapt to
the task over few episodes during adaptation phase to maximize the return in
the test phase. Instead of directly learning a meta-policy, we develop a
Meta-learner with Subtask Graph Inference(MSGI), which infers the latent
parameter of the task by interacting with the environment and maximizes the
return given the latent parameter. To facilitate learning, we adopt an
intrinsic reward inspired by upper confidence bound (UCB) that encourages
efficient exploration. Our experiment results on two grid-world domains and
StarCraft II environments show that the proposed method is able to accurately
infer the latent task parameter, and to adapt more efficiently than existing
meta RL and hierarchical RL methods.",Published in ICLR 2020,,,http://arxiv.org/abs/2001.00248v2,2020-04-14 01:44:03+00:00,2020-01-01 17:34:00+00:00,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2001.00248v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00248v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00248v2
A Comprehensive and Modularized Statistical Framework for Gradient Norm Equality in Deep Neural Networks,"['Zhaodong Chen', 'Lei Deng', 'Bangyan Wang', 'Guoqi Li', 'Yuan Xie']","In recent years, plenty of metrics have been proposed to identify networks
that are free of gradient explosion and vanishing. However, due to the
diversity of network components and complex serial-parallel hybrid connections
in modern DNNs, the evaluation of existing metrics usually requires strong
assumptions, complex statistical analysis, or has limited application fields,
which constraints their spread in the community. In this paper, inspired by the
Gradient Norm Equality and dynamical isometry, we first propose a novel metric
called Block Dynamical Isometry, which measures the change of gradient norm in
individual block. Because our Block Dynamical Isometry is norm-based, its
evaluation needs weaker assumptions compared with the original dynamical
isometry. To mitigate the challenging derivation, we propose a highly
modularized statistical framework based on free probability. Our framework
includes several key theorems to handle complex serial-parallel hybrid
connections and a library to cover the diversity of network components.
Besides, several sufficient prerequisites are provided. Powered by our metric
and framework, we analyze extensive initialization, normalization, and network
structures. We find that Gradient Norm Equality is a universal philosophy
behind them. Then, we improve some existing methods based on our analysis,
including an activation function selection strategy for initialization
techniques, a new configuration for weight normalization, and a depth-aware way
to derive coefficients in SeLU. Moreover, we propose a novel normalization
technique named second moment normalization, which is theoretically 30% faster
than batch normalization without accuracy loss. Last but not least, our
conclusions and methods are evidenced by extensive experiments on multiple
models over CIFAR10 and ImageNet.",Under review as a regular paper in TPAMI,"IEEE Transactions on Pattern Analysis and Machine Intelligence
  2020",10.1109/TPAMI.2020.3010201,http://arxiv.org/abs/2001.00254v1,2020-01-01 17:56:49+00:00,2020-01-01 17:56:49+00:00,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://dx.doi.org/10.1109/TPAMI.2020.3010201', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2001.00254v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00254v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00254v1
Fast Estimation of Information Theoretic Learning Descriptors using Explicit Inner Product Spaces,"['Kan Li', 'Jose C. Principe']","Kernel methods form a theoretically-grounded, powerful and versatile
framework to solve nonlinear problems in signal processing and machine
learning. The standard approach relies on the \emph{kernel trick} to perform
pairwise evaluations of a kernel function, leading to scalability issues for
large datasets due to its linear and superlinear growth with respect to the
training data. Recently, we proposed \emph{no-trick} (NT) kernel adaptive
filtering (KAF) that leverages explicit feature space mappings using
data-independent basis with constant complexity. The inner product defined by
the feature mapping corresponds to a positive-definite finite-rank kernel that
induces a finite-dimensional reproducing kernel Hilbert space (RKHS).
Information theoretic learning (ITL) is a framework where information theory
descriptors based on non-parametric estimator of Renyi entropy replace
conventional second-order statistics for the design of adaptive systems. An
RKHS for ITL defined on a space of probability density functions simplifies
statistical inference for supervised or unsupervised learning. ITL criteria
take into account the higher-order statistical behavior of the systems and
signals as desired. However, this comes at a cost of increased computational
complexity. In this paper, we extend the NT kernel concept to ITL for improved
information extraction from the signal without compromising scalability.
Specifically, we focus on a family of fast, scalable, and accurate estimators
for ITL using explicit inner product space (EIPS) kernels. We demonstrate the
superior performance of EIPS-ITL estimators and combined NT-KAF using EIPS-ITL
cost functions through experiments.","10 pages, 3 figures, 2 tables. arXiv admin note: text overlap with
  arXiv:1912.04530",,,http://arxiv.org/abs/2001.00265v1,2020-01-01 20:21:12+00:00,2020-01-01 20:21:12+00:00,cs.LG,"['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2001.00265v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00265v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00265v1
Options of Interest: Temporal Abstraction with Interest Functions,"['Khimya Khetarpal', 'Martin Klissarov', 'Maxime Chevalier-Boisvert', 'Pierre-Luc Bacon', 'Doina Precup']","Temporal abstraction refers to the ability of an agent to use behaviours of
controllers which act for a limited, variable amount of time. The options
framework describes such behaviours as consisting of a subset of states in
which they can initiate, an internal policy and a stochastic termination
condition. However, much of the subsequent work on option discovery has ignored
the initiation set, because of difficulty in learning it from data. We provide
a generalization of initiation sets suitable for general function
approximation, by defining an interest function associated with an option. We
derive a gradient-based learning algorithm for interest functions, leading to a
new interest-option-critic architecture. We investigate how interest functions
can be leveraged to learn interpretable and reusable temporal abstractions. We
demonstrate the efficacy of the proposed approach through quantitative and
qualitative results, in both discrete and continuous environments.","To appear in Proceedings of the Thirty-Fourth AAAI Conference on
  Artificial Intelligence (AAAI-20)",,,http://arxiv.org/abs/2001.00271v1,2020-01-01 21:24:39+00:00,2020-01-01 21:24:39+00:00,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2001.00271v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00271v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00271v1
Motivic clustering schemes for directed graphs,"['Facundo Mémoli', 'Guilherme Vituri F. Pinto']","Motivated by the concept of network motifs we construct certain clustering
methods (functors) which are parametrized by a given collection of motifs (or
representers).",23 pages,,,http://arxiv.org/abs/2001.00278v2,2020-01-06 16:37:03+00:00,2020-01-01 23:30:00+00:00,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2001.00278v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00278v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00278v2
Online Similarity Learning with Feedback for Invoice Line Item Matching,"['Chandresh Kumar Maurya', 'Neelamadhav Gantayat', 'Sampath Dechu', 'Tomas Horvath']","The procure to pay process (P2P) in large enterprises is a back-end business
process which deals with the procurement of products and services for
enterprise operations. Procurement is done by issuing purchase orders to
impaneled vendors and invoices submitted by vendors are paid after they go
through a rigorous validation process. Agents orchestrating P2P process often
encounter the problem of matching a product or service descriptions in the
invoice to those in purchase order and verify if the ordered items are what
have been supplied or serviced. For example, the description in the invoice and
purchase order could be TRES 739mL CD KER Smooth and TRES 0.739L CD KER Smth
which look different at word level but refer to the same item. In a typical P2P
process, agents are asked to manually select the products which are similar
before invoices are posted for payment. This step in the business process is
manual, repetitive, cumbersome, and costly. Since descriptions are not
well-formed sentences, we cannot apply existing semantic and syntactic text
similarity approaches directly. In this paper, we present two approaches to
solve the above problem using various types of available agent's recorded
feedback data. If the agent's feedback is in the form of a relative ranking
between descriptions, we use similarity ranking algorithm. If the agent's
feedback is absolute such as match or no-match, we use classification
similarity algorithm. We also present the threats to the validity of our
approach and present a possible remedy making use of product taxonomy and
catalog. We showcase the comparative effectiveness and efficiency of the
proposed approaches over many benchmarks and real-world data sets.",,"published as workshop paper in AAAI workshop on intelligent
  processing automation, 2020",,http://arxiv.org/abs/2001.00288v2,2020-02-14 14:26:30+00:00,2020-01-02 01:28:56+00:00,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2001.00288v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00288v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00288v2
Deep Learning for Learning Graph Representations,"['Wenwu Zhu', 'Xin Wang', 'Peng Cui']","Mining graph data has become a popular research topic in computer science and
has been widely studied in both academia and industry given the increasing
amount of network data in the recent years. However, the huge amount of network
data has posed great challenges for efficient analysis. This motivates the
advent of graph representation which maps the graph into a low-dimension vector
space, keeping original graph structure and supporting graph inference. The
investigation on efficient representation of a graph has profound theoretical
significance and important realistic meaning, we therefore introduce some basic
ideas in graph representation/network embedding as well as some representative
models in this chapter.","51 pages, 8 figures",,10.1007/978-3-030-31756-0_6,http://arxiv.org/abs/2001.00293v1,2020-01-02 02:13:28+00:00,2020-01-02 02:13:28+00:00,cs.LG,"['cs.LG', 'cs.IR', 'cs.SI', 'stat.ML']","[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-030-31756-0_6', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2001.00293v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00293v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00293v1
ATHENA: A Framework based on Diverse Weak Defenses for Building Adversarial Defense,"['Ying Meng', 'Jianhai Su', ""Jason O'Kane"", 'Pooyan Jamshidi']","There has been extensive research on developing defense techniques against
adversarial attacks; however, they have been mainly designed for specific model
families or application domains, therefore, they cannot be easily extended.
Based on the design philosophy of ensemble of diverse weak defenses, we propose
ATHENA---a flexible and extensible framework for building generic yet effective
defenses against adversarial attacks. We have conducted a comprehensive
empirical study to evaluate several realizations of ATHENA with four threat
models including zero-knowledge, black-box, gray-box, and white-box. We also
explain (i) why diversity matters, (ii) the generality of the defense
framework, and (iii) the overhead costs incurred by ATHENA.","18 pages, 32 figures",,,http://arxiv.org/abs/2001.00308v2,2020-10-16 21:11:24+00:00,2020-01-02 03:20:57+00:00,cs.LG,"['cs.LG', 'cs.CR', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2001.00308v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00308v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00308v2
On Consequentialism and Fairness,"['Dallas Card', 'Noah A. Smith']","Recent work on fairness in machine learning has primarily emphasized how to
define, quantify, and encourage ""fair"" outcomes. Less attention has been paid,
however, to the ethical foundations which underlie such efforts. Among the
ethical perspectives that should be taken into consideration is
consequentialism, the position that, roughly speaking, outcomes are all that
matter. Although consequentialism is not free from difficulties, and although
it does not necessarily provide a tractable way of choosing actions (because of
the combined problems of uncertainty, subjectivity, and aggregation), it
nevertheless provides a powerful foundation from which to critique the existing
literature on machine learning fairness. Moreover, it brings to the fore some
of the tradeoffs involved, including the problem of who counts, the pros and
cons of using a policy, and the relative value of the distant future. In this
paper we provide a consequentialist critique of common definitions of fairness
within machine learning, as well as a machine learning perspective on
consequentialism. We conclude with a broader discussion of the issues of
learning and randomization, which have important implications for the ethics of
automated decision making systems.",Updating to published version,"Front. Artif. Intell., 08 May 2020",10.3389/frai.2020.00034,http://arxiv.org/abs/2001.00329v2,2020-05-11 04:36:44+00:00,2020-01-02 05:39:48+00:00,cs.AI,"['cs.AI', 'cs.CY', 'cs.LG', 'stat.ML']","[arxiv.Result.Link('http://dx.doi.org/10.3389/frai.2020.00034', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2001.00329v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00329v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00329v2
Coarse-Grained Complexity for Dynamic Algorithms,"['Sayan Bhattacharya', 'Danupon Nanongkai', 'Thatchaphol Saranurak']","To date, the only way to argue polynomial lower bounds for dynamic algorithms
is via fine-grained complexity arguments. These arguments rely on strong
assumptions about specific problems such as the Strong Exponential Time
Hypothesis (SETH) and the Online Matrix-Vector Multiplication Conjecture (OMv).
While they have led to many exciting discoveries, dynamic algorithms still miss
out some benefits and lessons from the traditional ``coarse-grained'' approach
that relates together classes of problems such as P and NP. In this paper we
initiate the study of coarse-grained complexity theory for dynamic algorithms.
Below are among questions that this theory can answer.
  What if dynamic Orthogonal Vector (OV) is easy in the cell-probe model? A
research program for proving polynomial unconditional lower bounds for dynamic
OV in the cell-probe model is motivated by the fact that many conditional lower
bounds can be shown via reductions from the dynamic OV problem. Since the
cell-probe model is more powerful than word RAM and has historically allowed
smaller upper bounds, it might turn out that dynamic OV is easy in the
cell-probe model, making this research direction infeasible. Our theory implies
that if this is the case, there will be very interesting algorithmic
consequences: If dynamic OV can be maintained in polylogarithmic worst-case
update time in the cell-probe model, then so are several important dynamic
problems such as $k$-edge connectivity, $(1+\epsilon)$-approximate mincut,
$(1+\epsilon)$-approximate matching, planar nearest neighbors, Chan's subset
union and 3-vs-4 diameter. The same conclusion can be made when we replace
dynamic OV by, e.g., subgraph connectivity, single source reachability, Chan's
subset union, and 3-vs-4 diameter.
  Lower bounds for $k$-edge connectivity via dynamic OV? (see the full abstract
in the pdf file).",To be published at SODA 2020. The abstract is truncated,,,http://arxiv.org/abs/2001.00336v1,2020-01-02 06:14:54+00:00,2020-01-02 06:14:54+00:00,cs.CC,"['cs.CC', 'cs.DS']","[arxiv.Result.Link('http://arxiv.org/abs/2001.00336v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00336v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00336v1
"Visual Machine Learning: Insight through Eigenvectors, Chladni patterns and community detection in 2D particulate structures","['Raj Kishore', 'S. Swayamjyoti', 'Shreeja Das', 'Ajay K. Gogineni', 'Zohar Nussinov', 'D. Solenov', 'Kisor K. Sahu']","Machine learning (ML) is quickly emerging as a powerful tool with diverse
applications across an extremely broad spectrum of disciplines and commercial
endeavors. Typically, ML is used as a black box that provides little
illuminating rationalization of its output. In the current work, we aim to
better understand the generic intuition underlying unsupervised ML with a focus
on physical systems. The systems that are studied here as test cases comprise
of six different 2-dimensional (2-D) particulate systems of different
complexities. It is noted that the findings of this study are generic to any
unsupervised ML problem and are not restricted to materials systems alone.
Three rudimentary unsupervised ML techniques are employed on the adjacency
(connectivity) matrix of the six studied systems: (i) using principal
eigenvalue and eigenvectors of the adjacency matrix, (ii) spectral
decomposition, and (iii) a Potts model based community detection technique in
which a modularity function is maximized. We demonstrate that, while solving a
completely classical problem, ML technique produces features that are
distinctly connected to quantum mechanical solutions. Dissecting these features
help us to understand the deep connection between the classical non-linear
world and the quantum mechanical linear world through the kaleidoscope of ML
technique, which might have far reaching consequences both in the arena of
physical sciences and ML.",,,,http://arxiv.org/abs/2001.00345v1,2020-01-02 07:20:28+00:00,2020-01-02 07:20:28+00:00,cs.LG,"['cs.LG', 'cond-mat.soft', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2001.00345v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00345v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00345v1
Kernelized Support Tensor Train Machines,"['Cong Chen', 'Kim Batselier', 'Wenjian Yu', 'Ngai Wong']","Tensor, a multi-dimensional data structure, has been exploited recently in
the machine learning community. Traditional machine learning approaches are
vector- or matrix-based, and cannot handle tensorial data directly. In this
paper, we propose a tensor train (TT)-based kernel technique for the first
time, and apply it to the conventional support vector machine (SVM) for image
classification. Specifically, we propose a kernelized support tensor train
machine that accepts tensorial input and preserves the intrinsic kernel
property. The main contributions are threefold. First, we propose a TT-based
feature mapping procedure that maintains the TT structure in the feature space.
Second, we demonstrate two ways to construct the TT-based kernel function while
considering consistency with the TT inner product and preservation of
information. Third, we show that it is possible to apply different kernel
functions on different data modes. In principle, our method tensorizes the
standard SVM on its input structure and kernel mapping scheme. Extensive
experiments are performed on real-world tensor data, which demonstrates the
superiority of the proposed scheme under few-sample high-dimensional inputs.",,,,http://arxiv.org/abs/2001.00360v1,2020-01-02 08:40:15+00:00,2020-01-02 08:40:15+00:00,cs.LG,"['cs.LG', 'cs.CV', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2001.00360v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00360v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00360v1
Algorithmic Number On the Forehead Protocols Yielding Dense Ruzsa-Szemerédi Graphs and Hypergraphs,"['Noga Alon', 'Adi Shraibman']","We describe algorithmic Number On the Forehead protocols that provide dense
Ruzsa-Szemer\'{e}di graphs. One protocol leads to a simple and natural
extension of the original construction of Ruzsa and Szemer\'{e}di. The graphs
induced by this protocol have $n$ vertices, $\Omega(n^2/\log n)$ edges, and are
decomposable into $n^{1+O(1/\log \log n)}$ induced matchings. Another protocol
is an explicit (and slightly simpler) version of the construction of Alon,
Moitra and Sudakov, producing graphs with similar properties. We also
generalize the above protocols to more than three players, in order to
construct dense uniform hypergraphs in which every edge lies in a positive
small number of simplices.",,,,http://arxiv.org/abs/2001.00387v1,2020-01-02 10:51:34+00:00,2020-01-02 10:51:34+00:00,cs.CC,"['cs.CC', 'math.CO']","[arxiv.Result.Link('http://arxiv.org/abs/2001.00387v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00387v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00387v1
Inter- and Intra-domain Knowledge Transfer for Related Tasks in Deep Character Recognition,"['Nishai Kooverjee', 'Steven James', 'Terence van Zyl']","Pre-training a deep neural network on the ImageNet dataset is a common
practice for training deep learning models, and generally yields improved
performance and faster training times. The technique of pre-training on one
task and then retraining on a new one is called transfer learning. In this
paper we analyse the effectiveness of using deep transfer learning for
character recognition tasks. We perform three sets of experiments with varying
levels of similarity between source and target tasks to investigate the
behaviour of different types of knowledge transfer. We transfer both parameters
and features and analyse their behaviour. Our results demonstrate that no
significant advantage is gained by using a transfer learning approach over a
traditional machine learning approach for our character recognition tasks. This
suggests that using transfer learning does not necessarily presuppose a better
performing model in all cases.","To be published in SAUPEC/RobMech/PRASA 2020. Consists of 6 pages,
  with 6 figures",,,http://arxiv.org/abs/2001.00448v1,2020-01-02 14:18:25+00:00,2020-01-02 14:18:25+00:00,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2001.00448v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00448v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00448v1
Continuous-Discrete Reinforcement Learning for Hybrid Control in Robotics,"['Michael Neunert', 'Abbas Abdolmaleki', 'Markus Wulfmeier', 'Thomas Lampe', 'Jost Tobias Springenberg', 'Roland Hafner', 'Francesco Romano', 'Jonas Buchli', 'Nicolas Heess', 'Martin Riedmiller']","Many real-world control problems involve both discrete decision variables -
such as the choice of control modes, gear switching or digital outputs - as
well as continuous decision variables - such as velocity setpoints, control
gains or analogue outputs. However, when defining the corresponding optimal
control or reinforcement learning problem, it is commonly approximated with
fully continuous or fully discrete action spaces. These simplifications aim at
tailoring the problem to a particular algorithm or solver which may only
support one type of action space. Alternatively, expert heuristics are used to
remove discrete actions from an otherwise continuous space. In contrast, we
propose to treat hybrid problems in their 'native' form by solving them with
hybrid reinforcement learning, which optimizes for discrete and continuous
actions simultaneously. In our experiments, we first demonstrate that the
proposed approach efficiently solves such natively hybrid reinforcement
learning problems. We then show, both in simulation and on robotic hardware,
the benefits of removing possibly imperfect expert-designed heuristics. Lastly,
hybrid reinforcement learning encourages us to rethink problem definitions. We
propose reformulating control problems, e.g. by adding meta actions, to improve
exploration or reduce mechanical wear and tear.","Presented at the 3rd Conference on Robot Learning (CoRL 2019), Osaka,
  Japan. Video: https://youtu.be/eUqQDLQXb7I",,,http://arxiv.org/abs/2001.00449v1,2020-01-02 14:19:33+00:00,2020-01-02 14:19:33+00:00,cs.LG,"['cs.LG', 'cs.RO', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2001.00449v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00449v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00449v1
Reasoning on Knowledge Graphs with Debate Dynamics,"['Marcel Hildebrandt', 'Jorge Andres Quintero Serna', 'Yunpu Ma', 'Martin Ringsquandl', 'Mitchell Joblin', 'Volker Tresp']","We propose a novel method for automatic reasoning on knowledge graphs based
on debate dynamics. The main idea is to frame the task of triple classification
as a debate game between two reinforcement learning agents which extract
arguments -- paths in the knowledge graph -- with the goal to promote the fact
being true (thesis) or the fact being false (antithesis), respectively. Based
on these arguments, a binary classifier, called the judge, decides whether the
fact is true or false. The two agents can be considered as sparse, adversarial
feature generators that present interpretable evidence for either the thesis or
the antithesis. In contrast to other black-box methods, the arguments allow
users to get an understanding of the decision of the judge. Since the focus of
this work is to create an explainable method that maintains a competitive
predictive accuracy, we benchmark our method on the triple classification and
link prediction task. Thereby, we find that our method outperforms several
baselines on the benchmark datasets FB15k-237, WN18RR, and Hetionet. We also
conduct a survey and find that the extracted arguments are informative for
users.",AAAI-2020,,,http://arxiv.org/abs/2001.00461v1,2020-01-02 14:44:23+00:00,2020-01-02 14:44:23+00:00,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2001.00461v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00461v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00461v1
Thresholds of descending algorithms in inference problems,"['Stefano Sarao Mannelli', 'Lenka Zdeborova']","We review recent works on analyzing the dynamics of gradient-based algorithms
in a prototypical statistical inference problem. Using methods and insights
from the physics of glassy systems, these works showed how to understand
quantitatively and qualitatively the performance of gradient-based algorithms.
Here we review the key results and their interpretation in non-technical terms
accessible to a wide audience of physicists in the context of related works.","8 pages, 4 figures",J. Stat. Mech. (2020) 034004,10.1088/1742-5468/ab7123,http://arxiv.org/abs/2001.00479v2,2020-01-04 08:53:20+00:00,2020-01-02 15:08:40+00:00,cs.LG,"['cs.LG', 'cond-mat.dis-nn', 'stat.ML']","[arxiv.Result.Link('http://dx.doi.org/10.1088/1742-5468/ab7123', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2001.00479v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00479v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00479v2
Reject Illegal Inputs with Generative Classifier Derived from Any Discriminative Classifier,['Xin Wang'],"Generative classifiers have been shown promising to detect illegal inputs
including adversarial examples and out-of-distribution samples. Supervised Deep
Infomax~(SDIM) is a scalable end-to-end framework to learn generative
classifiers. In this paper, we propose a modification of SDIM termed
SDIM-\emph{logit}. Instead of training generative classifier from scratch,
SDIM-\emph{logit} first takes as input the logits produced any given
discriminative classifier, and generate logit representations; then a
generative classifier is derived by imposing statistical constraints on logit
representations. SDIM-\emph{logit} could inherit the performance of the
discriminative classifier without loss. SDIM-\emph{logit} incurs a negligible
number of additional parameters, and can be efficiently trained with base
classifiers fixed. We perform \emph{classification with rejection}, where test
samples whose class conditionals are smaller than pre-chosen thresholds will be
rejected without predictions. Experiments on illegal inputs, including
adversarial examples, samples with common corruptions, and
out-of-distribution~(OOD) samples show that allowed to reject a portion of test
samples, SDIM-\emph{logit} significantly improves the performance on the left
test sets.","7 pages, 3 figures",,,http://arxiv.org/abs/2001.00483v1,2020-01-02 15:11:58+00:00,2020-01-02 15:11:58+00:00,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2001.00483v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00483v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00483v1
Uncertainty-Based Out-of-Distribution Classification in Deep Reinforcement Learning,"['Andreas Sedlmeier', 'Thomas Gabor', 'Thomy Phan', 'Lenz Belzner', 'Claudia Linnhoff-Popien']","Robustness to out-of-distribution (OOD) data is an important goal in building
reliable machine learning systems. Especially in autonomous systems, wrong
predictions for OOD inputs can cause safety critical situations. As a first
step towards a solution, we consider the problem of detecting such data in a
value-based deep reinforcement learning (RL) setting. Modelling this problem as
a one-class classification problem, we propose a framework for
uncertainty-based OOD classification: UBOOD. It is based on the effect that an
agent's epistemic uncertainty is reduced for situations encountered during
training (in-distribution), and thus lower than for unencountered (OOD)
situations. Being agnostic towards the approach used for estimating epistemic
uncertainty, combinations with different uncertainty estimation methods, e.g.
approximate Bayesian inference methods or ensembling techniques are possible.
We further present a first viable solution for calculating a dynamic
classification threshold, based on the uncertainty distribution of the training
data. Evaluation shows that the framework produces reliable classification
results when combined with ensemble-based estimators, while the combination
with concrete dropout-based estimators fails to reliably detect OOD situations.
In summary, UBOOD presents a viable approach for OOD classification in deep RL
settings by leveraging the epistemic uncertainty of the agent's value function.",arXiv admin note: text overlap with arXiv:1901.02219,"Proceedings of the 12th International Conference on Agents and
  Artificial Intelligence - Volume 2: ICAART, 2020, ISBN 978-989-758-395-7,
  pages 522-529",10.5220/0008949905220529,http://arxiv.org/abs/2001.00496v1,2019-12-31 09:52:49+00:00,2019-12-31 09:52:49+00:00,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Link('http://dx.doi.org/10.5220/0008949905220529', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2001.00496v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00496v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2001.00496v1
Etat de l'art sur l'application des bandits multi-bras,['Djallel Bouneffouf'],"The Multi-armed bandit offer the advantage to learn and exploit the already
learnt knowledge at the same time. This capability allows this approach to be
applied in different domains, going from clinical trials where the goal is
investigating the effects of different experimental treatments while minimizing
patient losses, to adaptive routing where the goal is to minimize the delays in
a network. This article provides a review of the recent results on applying
bandit to real-life scenario and summarize the state of the art for each of
these fields. Different techniques has been proposed to solve this problem
setting, like epsilon-greedy, Upper confident bound (UCB) and Thompson Sampling
(TS). We are showing here how this algorithms were adapted to solve the
different problems of exploration exploitation.",in French,,,http://arxiv.org/abs/2101.00001v1,2021-01-04 18:12:28+00:00,2021-01-04 18:12:28+00:00,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2101.00001v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00001v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2101.00001v1
Automatic-differentiated Physics-Informed Echo State Network (API-ESN),"['Alberto Racca', 'Luca Magri']","We propose the Automatic-differentiated Physics-Informed Echo State Network
(API-ESN). The network is constrained by the physical equations through the
reservoir's exact time-derivative, which is computed by automatic
differentiation. As compared to the original Physics-Informed Echo State
Network, the accuracy of the time-derivative is increased by up to seven orders
of magnitude. This increased accuracy is key in chaotic dynamical systems,
where errors grows exponentially in time. The network is showcased in the
reconstruction of unmeasured (hidden) states of a chaotic system. The API-ESN
eliminates a source of error, which is present in existing physics-informed
echo state networks, in the computation of the time-derivative. This opens up
new possibilities for an accurate reconstruction of chaotic dynamical states.","7 pages, 3 figures",,,http://arxiv.org/abs/2101.00002v2,2021-03-24 16:33:42+00:00,2020-12-28 20:44:17+00:00,cs.LG,"['cs.LG', 'nlin.CD']","[arxiv.Result.Link('http://arxiv.org/abs/2101.00002v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00002v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2101.00002v2
Yet another argument in favour of NP=CoNP,['Edward Hermann Haeusler'],"This article shows yet another proof of NP=CoNP$. In a previous article, we
proved that NP=PSPACE and from it we can conclude that NP=CoNP immediately. The
former proof shows how to obtain polynomial and, polynomial in time checkable
Dag-like proofs for all purely implicational Minimal logic tautologies. From
the fact that Minimal implicational logic is PSPACE-complete we get the proof
that NP=PSPACE.
  This first proof of NP=CoNP uses Hudelmaier linear upper-bound on the height
of Sequent Calculus minimal implicational logic proofs. In an addendum to the
proof of NP=PSPACE, we observe that we do not need to use Hudelmaier
upper-bound since any proof of non-hamiltonicity for any graph is linear
upper-bounded. By the CoNP-completeness of non-hamiltonicity, we obtain NP=CoNP
as a corollary of the first proof. In this article we show the third proof of
CoNP=NP, also providing polynomial size and polynomial verifiable certificates
that are Dags. They are generated from normal Natural Deduction proofs, linear
height upper-bounded too, by removing redundancy, i.e., repeated parts. The
existence of repeated parts is a consequence of the redundancy theorem for a
family of super-polynomial proofs in the purely implicational Minimal logic. It
is mandatory to read at least two previous articles to get the details of the
proof presented here. The article that proves the redundancy theorem and the
article that shows how to remove the repeated parts of a normal Natural
Deduction proof to have a polynomial Dag certificate for minimal implicational
logic tautologies.","This article puts together the results shown in arXiv:2009.09802v1
  and in arXiv:2012.07833v1 to show a proof of NP=CoNP. It is need to read
  these article to get the details on the proof presented here",,,http://arxiv.org/abs/2101.00003v1,2020-12-28 22:08:20+00:00,2020-12-28 22:08:20+00:00,cs.CC,"['cs.CC', 'cs.LO']","[arxiv.Result.Link('http://arxiv.org/abs/2101.00003v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00003v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2101.00003v1
Random Embeddings with Optimal Accuracy,['Maciej Skorski'],"This work constructs Jonson-Lindenstrauss embeddings with best accuracy, as
measured by variance, mean-squared error and exponential concentration of the
length distortion. Lower bounds for any data and embedding dimensions are
determined, and accompanied by matching and efficiently samplable constructions
(built on orthogonal matrices). Novel techniques: a unit sphere
parametrization, the use of singular-value latent variables and Schur-convexity
are of independent interest.",,,,http://arxiv.org/abs/2101.00029v1,2020-12-31 19:00:31+00:00,2020-12-31 19:00:31+00:00,cs.LG,"['cs.LG', 'math.PR', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2101.00029v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00029v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2101.00029v1
Modified Gaussian Process Regression Models for Cyclic Capacity Prediction of Lithium-ion Batteries,"['Kailong Liu', 'Xiaosong Hu', 'Zhongbao Wei', 'Yi Li', 'Yan Jiang']","This paper presents the development of machine learning-enabled data-driven
models for effective capacity predictions for lithium-ion batteries under
different cyclic conditions. To achieve this, a model structure is first
proposed with the considerations of battery ageing tendency and the
corresponding operational temperature and depth-of-discharge. Then based on a
systematic understanding of covariance functions within the Gaussian process
regression, two related data-driven models are developed. Specifically, by
modifying the isotropic squared exponential kernel with an automatic relevance
determination structure, 'Model A' could extract the highly relevant input
features for capacity predictions. Through coupling the Arrhenius law and a
polynomial equation into a compositional kernel, 'Model B' is capable of
considering the electrochemical and empirical knowledge of battery degradation.
The developed models are validated and compared on the Nickel Manganese Cobalt
Oxide (NMC) lithium-ion batteries with various cycling patterns. Experimental
results demonstrate that the modified Gaussian process regression model
considering the battery electrochemical and empirical ageing signature
outperforms other counterparts and is able to achieve satisfactory results for
both one-step and multi-step predictions. The proposed technique is promising
for battery capacity predictions under various cycling cases.",,,,http://arxiv.org/abs/2101.00035v1,2020-12-31 19:05:27+00:00,2020-12-31 19:05:27+00:00,cs.LG,"['cs.LG', 'cs.SY', 'eess.SY']","[arxiv.Result.Link('http://arxiv.org/abs/2101.00035v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00035v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2101.00035v1
Optimizing Optimizers: Regret-optimal gradient descent algorithms,"['Philippe Casgrain', 'Anastasis Kratsios']","The need for fast and robust optimization algorithms are of critical
importance in all areas of machine learning. This paper treats the task of
designing optimization algorithms as an optimal control problem. Using regret
as a metric for an algorithm's performance, we study the existence, uniqueness
and consistency of regret-optimal algorithms. By providing first-order
optimality conditions for the control problem, we show that regret-optimal
algorithms must satisfy a specific structure in their dynamics which we show is
equivalent to performing dual-preconditioned gradient descent on the value
function generated by its regret. Using these optimal dynamics, we provide
bounds on their rates of convergence to solutions of convex optimization
problems. Though closed-form optimal dynamics cannot be obtained in general, we
present fast numerical methods for approximating them, generating optimization
algorithms which directly optimize their long-term regret. Lastly, these are
benchmarked against commonly used optimization algorithms to demonstrate their
effectiveness.","12 pages body, 42 pages total, 2 figures",,,http://arxiv.org/abs/2101.00041v2,2021-01-19 22:50:56+00:00,2020-12-31 19:13:53+00:00,cs.LG,"['cs.LG', 'math.OC', '49K10, 49J10, 49M05, 49K27, 49J50, 65K10', 'F.2.0']","[arxiv.Result.Link('http://arxiv.org/abs/2101.00041v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00041v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2101.00041v2
Federated Nonconvex Sparse Learning,"['Qianqian Tong', 'Guannan Liang', 'Tan Zhu', 'Jinbo Bi']","Nonconvex sparse learning plays an essential role in many areas, such as
signal processing and deep network compression. Iterative hard thresholding
(IHT) methods are the state-of-the-art for nonconvex sparse learning due to
their capability of recovering true support and scalability with large
datasets. Theoretical analysis of IHT is currently based on centralized IID
data. In realistic large-scale situations, however, data are distributed,
hardly IID, and private to local edge computing devices. It is thus necessary
to examine the property of IHT in federated settings, which update in parallel
on local devices and communicate with a central server only once in a while
without sharing local data.
  In this paper, we propose two IHT methods: Federated Hard Thresholding
(Fed-HT) and Federated Iterative Hard Thresholding (FedIter-HT). We prove that
both algorithms enjoy a linear convergence rate and have strong guarantees to
recover the optimal sparse estimator, similar to traditional IHT methods, but
now with decentralized non-IID data. Empirical results demonstrate that the
Fed-HT and FedIter-HT outperform their competitor - a distributed IHT, in terms
of decreasing the objective values with lower requirements on communication
rounds and bandwidth.",,,,http://arxiv.org/abs/2101.00052v1,2020-12-31 19:43:45+00:00,2020-12-31 19:43:45+00:00,cs.LG,['cs.LG'],"[arxiv.Result.Link('http://arxiv.org/abs/2101.00052v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00052v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2101.00052v1
Data Criticality in Multi-Threaded Applications: An Insight for Many-Core Systems,"['Abhijit Das', 'John Jose', 'Prabhat Mishra']","Multi-threaded applications are capable of exploiting the full potential of
many-core systems. However, Network-on-Chip (NoC) based inter-core
communication in many-core systems is responsible for 60-75% of the miss
latency experienced by multi-threaded applications. Delay in the arrival of
critical data at the requesting core severely hampers performance. This brief
presents some interesting insights about how critical data is requested from
the memory by multi-threaded applications. Then it investigates the cause of
delay in NoC and how it affects the performance. Finally, this brief shows how
NoC-aware memory access optimisations can significantly improve performance.
Our experimental evaluation considers early restart memory access optimisation
and demonstrates that by exploiting NoC resources, critical data can be
prioritised to reduce miss penalty by 10-12% and improve system performance by
7-11%.",,,,http://arxiv.org/abs/2101.00055v1,2020-12-31 20:13:10+00:00,2020-12-31 20:13:10+00:00,cs.AR,['cs.AR'],"[arxiv.Result.Link('http://arxiv.org/abs/2101.00055v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00055v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2101.00055v1
Conflict-driven Inductive Logic Programming,['Mark Law'],"The goal of Inductive Logic Programming (ILP) is to learn a program that
explains a set of examples. Until recently, most research on ILP targeted
learning Prolog programs. The ILASP system instead learns Answer Set Programs
(ASP). Learning such expressive programs widens the applicability of ILP
considerably; for example, enabling preference learning, learning common-sense
knowledge, including defaults and exceptions, and learning non-deterministic
theories.
  Early versions of ILASP can be considered meta-level ILP approaches, which
encode a learning task as a logic program and delegate the search to an ASP
solver. More recently, ILASP has shifted towards a new method, inspired by
conflict-driven SAT and ASP solvers. The fundamental idea of the approach,
called Conflict-driven ILP (CDILP), is to iteratively interleave the search for
a hypothesis with the generation of constraints which explain why the current
hypothesis does not cover a particular example. These coverage constraints
allow ILASP to rule out not just the current hypothesis, but an entire class of
hypotheses that do not satisfy the coverage constraint.
  This paper formalises the CDILP approach and presents the ILASP3 and ILASP4
systems for CDILP, which are demonstrated to be more scalable than previous
ILASP systems, particularly in the presence of noise.
  Under consideration in Theory and Practice of Logic Programming (TPLP).","Under consideration in Theory and Practice of Logic Programming
  (TPLP)",,,http://arxiv.org/abs/2101.00058v3,2022-01-14 19:21:47+00:00,2020-12-31 20:24:28+00:00,cs.AI,['cs.AI'],"[arxiv.Result.Link('http://arxiv.org/abs/2101.00058v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00058v3', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2101.00058v3
Explicit regularization and implicit bias in deep network classifiers trained with the square loss,"['Tomaso Poggio', 'Qianli Liao']","Deep ReLU networks trained with the square loss have been observed to perform
well in classification tasks. We provide here a theoretical justification based
on analysis of the associated gradient flow. We show that convergence to a
solution with the absolute minimum norm is expected when normalization
techniques such as Batch Normalization (BN) or Weight Normalization (WN) are
used together with Weight Decay (WD). The main property of the minimizers that
bounds their expected error is the norm: we prove that among all the
close-to-interpolating solutions, the ones associated with smaller Frobenius
norms of the unnormalized weight matrices have better margin and better bounds
on the expected classification error. With BN but in the absence of WD, the
dynamical system is singular. Implicit dynamical regularization -- that is
zero-initial conditions biasing the dynamics towards high margin solutions --
is also possible in the no-BN and no-WD case. The theory yields several
predictions, including the role of BN and weight decay, aspects of Papyan, Han
and Donoho's Neural Collapse and the constraints induced by BN on the network
weights.",,,,http://arxiv.org/abs/2101.00072v1,2020-12-31 21:07:56+00:00,2020-12-31 21:07:56+00:00,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2101.00072v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00072v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2101.00072v1
Bosonic Random Walk Networks for Graph Learning,"['Shiv Shankar', 'Don Towsley']","The development of Graph Neural Networks (GNNs) has led to great progress in
machine learning on graph-structured data. These networks operate via diffusing
information across the graph nodes while capturing the structure of the graph.
Recently there has also seen tremendous progress in quantum computing
techniques. In this work, we explore applications of multi-particle quantum
walks on diffusing information across graphs. Our model is based on learning
the operators that govern the dynamics of quantum random walkers on graphs. We
demonstrate the effectiveness of our method on classification and regression
tasks.",,,,http://arxiv.org/abs/2101.00082v1,2020-12-31 21:40:40+00:00,2020-12-31 21:40:40+00:00,cs.LG,['cs.LG'],"[arxiv.Result.Link('http://arxiv.org/abs/2101.00082v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00082v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2101.00082v1
DVD: A Diagnostic Dataset for Multi-step Reasoning in Video Grounded Dialogue,"['Hung Le', 'Chinnadhurai Sankar', 'Seungwhan Moon', 'Ahmad Beirami', 'Alborz Geramifard', 'Satwik Kottur']","A video-grounded dialogue system is required to understand both dialogue,
which contains semantic dependencies from turn to turn, and video, which
contains visual cues of spatial and temporal scene variations. Building such
dialogue systems is a challenging problem, involving various reasoning types on
both visual and language inputs. Existing benchmarks do not have enough
annotations to thoroughly analyze dialogue systems and understand their
capabilities and limitations in isolation. These benchmarks are also not
explicitly designed to minimise biases that models can exploit without actual
reasoning. To address these limitations, in this paper, we present DVD, a
Diagnostic Dataset for Video-grounded Dialogues. The dataset is designed to
contain minimal biases and has detailed annotations for the different types of
reasoning over the spatio-temporal space of video. Dialogues are synthesized
over multiple question turns, each of which is injected with a set of
cross-turn semantic relationships. We use DVD to analyze existing approaches,
providing interesting insights into their abilities and limitations. In total,
DVD is built from $11k$ CATER synthetic videos and contains $10$ instances of
$10$-round dialogues for each video, resulting in more than $100k$ dialogues
and $1M$ question-answer pairs. Our code and dataset are publicly available at
https://github.com/facebookresearch/DVDialogues.","20 pages, 14 figures, 8 tables",Association for Computational Linguistics (2021),,http://arxiv.org/abs/2101.00151v2,2021-06-14 15:55:57+00:00,2021-01-01 03:20:22+00:00,cs.AI,"['cs.AI', 'cs.CL', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2101.00151v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00151v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2101.00151v2
Active Learning Under Malicious Mislabeling and Poisoning Attacks,"['Jing Lin', 'Ryan Luley', 'Kaiqi Xiong']","Deep neural networks usually require large labeled datasets for training to
achieve state-of-the-art performance in many tasks, such as image
classification and natural language processing. Although a lot of data is
created each day by active Internet users, most of these data are unlabeled and
are vulnerable to data poisoning attacks. In this paper, we develop an
efficient active learning method that requires fewer labeled instances and
incorporates the technique of adversarial retraining in which additional
labeled artificial data are generated without increasing the budget of the
labeling. The generated adversarial examples also provide a way to measure the
vulnerability of the model. To check the performance of the proposed method
under an adversarial setting, i.e., malicious mislabeling and data poisoning
attacks, we perform an extensive evaluation on the reduced CIFAR-10 dataset,
which contains only two classes: airplane and frog. Our experimental results
demonstrate that the proposed active learning method is efficient for defending
against malicious mislabeling and data poisoning attacks. Specifically, whereas
the baseline active learning method based on the random sampling strategy
performs poorly (about 50%) under a malicious mislabeling attack, the proposed
active learning method can achieve the desired accuracy of 89% using only
one-third of the dataset on average.",2021 IEEE Global Communications Conference (GLOBECOM),,,http://arxiv.org/abs/2101.00157v4,2021-09-02 04:12:13+00:00,2021-01-01 03:43:36+00:00,cs.LG,"['cs.LG', 'cs.CR', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2101.00157v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00157v4', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2101.00157v4
Fidel: Reconstructing Private Training Samples from Weight Updates in Federated Learning,"['David Enthoven', 'Zaid Al-Ars']","With the increasing number of data collectors such as smartphones, immense
amounts of data are available. Federated learning was developed to allow for
distributed learning on a massive scale whilst still protecting each users'
privacy. This privacy is claimed by the notion that the centralized server does
not have any access to a client's data, solely the client's model update. In
this paper, we evaluate a novel attack method within regular federated learning
which we name the First Dense Layer Attack (Fidel). The methodology of using
this attack is discussed, and as a proof of viability we show how this attack
method can be used to great effect for densely connected networks and
convolutional neural networks. We evaluate some key design decisions and show
that the usage of ReLu and Dropout are detrimental to the privacy of a client's
local dataset. We show how to recover on average twenty out of thirty private
data samples from a client's model update employing a fully connected neural
network with very little computational resources required. Similarly, we show
that over thirteen out of twenty samples can be recovered from a convolutional
neural network update. An open source implementation of this attack can be
found here https://github.com/Davidenthoven/Fidel-Reconstruction-Demo",,,,http://arxiv.org/abs/2101.00159v2,2022-04-08 23:45:46+00:00,2021-01-01 04:00:23+00:00,cs.LG,['cs.LG'],"[arxiv.Result.Link('http://arxiv.org/abs/2101.00159v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00159v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2101.00159v2
Early Prediction of Heart Disease Using PCA and Hybrid Genetic Algorithm with k-Means,"['Md. Touhidul Islam', 'Sanjida Reza Rafa', 'Md. Golam Kibria']","Worldwide research shows that millions of lives lost per year because of
heart disease. The healthcare sector produces massive volumes of data on heart
disease that are sadly not used to locate secret knowledge for successful
decision making. One of the most important aspects at this moment is detecting
heart disease at an early stage. Researchers have applied distinct techniques
to the UCI Machine Learning heart disease dataset. Many researchers have tried
to apply some complex techniques to this dataset, where detailed studies are
still missing. In this paper, Principal Component Analysis (PCA) has been used
to reduce attributes. Apart from a Hybrid genetic algorithm (HGA) with k-means
used for final clustering. Typically, the k-means method is using for
clustering the data. This type of clustering can get stuck in the local optima
because this method is heuristic. We used the Hybrid Genetic Algorithm (HGA)
for data clustering to avoid this problem. Our proposed method can predict
early heart disease with an accuracy of 94.06%.","6 pages, 9 figures, Presented in the Proceedings of the 23rd
  International Conference on Computer and Information Technology (ICCIT),
  19-21 December, 2020, Dhaka, Bangladesh",,,http://arxiv.org/abs/2101.00183v1,2021-01-01 07:14:38+00:00,2021-01-01 07:14:38+00:00,cs.LG,['cs.LG'],"[arxiv.Result.Link('http://arxiv.org/abs/2101.00183v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00183v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2101.00183v1
Inverse reinforcement learning for autonomous navigation via differentiable semantic mapping and planning,"['Tianyu Wang', 'Vikas Dhiman', 'Nikolay Atanasov']","This paper focuses on inverse reinforcement learning for autonomous
navigation using distance and semantic category observations. The objective is
to infer a cost function that explains demonstrated behavior while relying only
on the expert's observations and state-control trajectory. We develop a map
encoder, that infers semantic category probabilities from the observation
sequence, and a cost encoder, defined as a deep neural network over the
semantic features. Since the expert cost is not directly observable, the model
parameters can only be optimized by differentiating the error between
demonstrated controls and a control policy computed from the cost estimate. We
propose a new model of expert behavior that enables error minimization using a
closed-form subgradient computed only over a subset of promising states via a
motion planning algorithm. Our approach allows generalizing the learned
behavior to new environments with new spatial configurations of the semantic
categories. We analyze the different components of our model in a minigrid
environment. We also demonstrate that our approach learns to follow traffic
rules in the autonomous driving CARLA simulator by relying on semantic
observations of buildings, sidewalks, and road lanes.","16 pages, 12 figures. arXiv admin note: text overlap with
  arXiv:2006.05043",,,http://arxiv.org/abs/2101.00186v1,2021-01-01 07:41:08+00:00,2021-01-01 07:41:08+00:00,cs.LG,"['cs.LG', 'cs.RO']","[arxiv.Result.Link('http://arxiv.org/abs/2101.00186v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00186v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2101.00186v1
B-SMALL: A Bayesian Neural Network approach to Sparse Model-Agnostic Meta-Learning,"['Anish Madan', 'Ranjitha Prasad']","There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.",,,,http://arxiv.org/abs/2101.00203v1,2021-01-01 09:19:48+00:00,2021-01-01 09:19:48+00:00,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV']","[arxiv.Result.Link('http://arxiv.org/abs/2101.00203v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00203v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2101.00203v1
An iterative K-FAC algorithm for Deep Learning,['Yingshi Chen'],"Kronecker-factored Approximate Curvature (K-FAC) method is a high efficiency
second order optimizer for the deep learning. Its training time is less than
SGD(or other first-order method) with same accuracy in many large-scale
problems. The key of K-FAC is to approximates Fisher information matrix (FIM)
as a block-diagonal matrix where each block is an inverse of tiny Kronecker
factors. In this short note, we present CG-FAC -- an new iterative K-FAC
algorithm. It uses conjugate gradient method to approximate the nature
gradient. This CG-FAC method is matrix-free, that is, no need to generate the
FIM matrix, also no need to generate the Kronecker factors A and G. We prove
that the time and memory complexity of iterative CG-FAC is much less than that
of standard K-FAC algorithm.",5 pages,,,http://arxiv.org/abs/2101.00218v1,2021-01-01 12:04:01+00:00,2021-01-01 12:04:01+00:00,cs.LG,"['cs.LG', 'cs.NA', 'math.NA', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2101.00218v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00218v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2101.00218v1
Adam revisited: a weighted past gradients perspective,"['Hui Zhong', 'Zaiyi Chen', 'Chuan Qin', 'Zai Huang', 'Vincent W. Zheng', 'Tong Xu', 'Enhong Chen']","Adaptive learning rate methods have been successfully applied in many fields,
especially in training deep neural networks. Recent results have shown that
adaptive methods with exponential increasing weights on squared past gradients
(i.e., ADAM, RMSPROP) may fail to converge to the optimal solution. Though many
algorithms, such as AMSGRAD and ADAMNC, have been proposed to fix the
non-convergence issues, achieving a data-dependent regret bound similar to or
better than ADAGRAD is still a challenge to these methods. In this paper, we
propose a novel adaptive method weighted adaptive algorithm (WADA) to tackle
the non-convergence issues. Unlike AMSGRAD and ADAMNC, we consider using a
milder growing weighting strategy on squared past gradient, in which weights
grow linearly. Based on this idea, we propose weighted adaptive gradient method
framework (WAGMF) and implement WADA algorithm on this framework. Moreover, we
prove that WADA can achieve a weighted data-dependent regret bound, which could
be better than the original regret bound of ADAGRAD when the gradients decrease
rapidly. This bound may partially explain the good performance of ADAM in
practice. Finally, extensive experiments demonstrate the effectiveness of WADA
and its variants in comparison with several variants of ADAM on training convex
problems and deep neural networks.","Zhong, Hui, et al. ""Adam revisited: a weighted past gradients
  perspective."" Frontiers of Computer Science 14.5 (2020): 1-16","Front. Comput. Sci. 14, 145309 (2020)",10.1007/s11704-019-8457-x,http://arxiv.org/abs/2101.00238v1,2021-01-01 14:01:52+00:00,2021-01-01 14:01:52+00:00,cs.LG,"['cs.LG', 'math.OC']","[arxiv.Result.Link('http://dx.doi.org/10.1007/s11704-019-8457-x', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2101.00238v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00238v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2101.00238v1
A General Counterexample to Any Decision Theory and Some Responses,['Joar Skalse'],"In this paper I present an argument and a general schema which can be used to
construct a problem case for any decision theory, in a way that could be taken
to show that one cannot formulate a decision theory that is never outperformed
by any other decision theory. I also present and discuss a number of possible
responses to this argument. One of these responses raises the question of what
it means for two decision problems to be ""equivalent"" in the relevant sense,
and gives an answer to this question which would invalidate the first argument.
However, this position would have further consequences for how we compare
different decision theories in decision problems already discussed in the
literature (including e.g. Newcomb's problem).",4 pages,,,http://arxiv.org/abs/2101.00280v1,2021-01-01 17:47:11+00:00,2021-01-01 17:47:11+00:00,cs.AI,['cs.AI'],"[arxiv.Result.Link('http://arxiv.org/abs/2101.00280v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00280v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2101.00280v1
An Ontology Design Pattern for representing Recurrent Situations,"['Valentina Anita Carriero', 'Aldo Gangemi', 'Andrea Giovanni Nuzzolese', 'Valentina Presutti']","In this paper, we present an Ontology Design Pattern for representing
situations that recur at regular periods and share some invariant factors,
which unify them conceptually: we refer to this set of recurring situations as
recurrent situation series. The proposed pattern appears to be foundational,
since it can be generalised for modelling the top-level domain-independent
concept of recurrence, which is strictly associated with invariance. The
pattern reuses other foundational patterns such as Collection, Description and
Situation, Classification, Sequence. Indeed, a recurrent situation series is
formalised as both a collection of situations occurring regularly over time and
unified according to some properties that are common to all the members, and a
situation itself, which provides a relational context to its members that
satisfy a reference description. Besides including some exemplifying instances
of this pattern, we show how it has been implemented and specialised to model
recurrent cultural events and ceremonies in ArCo, the Knowledge Graph of
Italian cultural heritage.",,,10.3233/SSW210013,http://arxiv.org/abs/2101.00286v1,2021-01-01 18:20:13+00:00,2021-01-01 18:20:13+00:00,cs.AI,['cs.AI'],"[arxiv.Result.Link('http://dx.doi.org/10.3233/SSW210013', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2101.00286v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00286v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2101.00286v1
When Is Generalizable Reinforcement Learning Tractable?,"['Dhruv Malik', 'Yuanzhi Li', 'Pradeep Ravikumar']","Agents trained by reinforcement learning (RL) often fail to generalize beyond
the environment they were trained in, even when presented with new scenarios
that seem similar to the training environment. We study the query complexity
required to train RL agents that generalize to multiple environments.
Intuitively, tractable generalization is only possible when the environments
are similar or close in some sense. To capture this, we introduce Weak
Proximity, a natural structural condition that requires the environments to
have highly similar transition and reward functions and share a policy
providing optimal value. Despite such shared structure, we prove that tractable
generalization is impossible in the worst case. This holds even when each
individual environment can be efficiently solved to obtain an optimal linear
policy, and when the agent possesses a generative model. Our lower bound
applies to the more complex task of representation learning for the purpose of
efficient generalization to multiple environments. On the positive side, we
introduce Strong Proximity, a strengthened condition which we prove is
sufficient for efficient generalization.","Neurips 2021, v3 fixes minor typos",,,http://arxiv.org/abs/2101.00300v3,2021-10-25 21:55:59+00:00,2021-01-01 19:08:24+00:00,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2101.00300v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00300v3', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2101.00300v3
Neural Architecture Search via Combinatorial Multi-Armed Bandit,"['Hanxun Huang', 'Xingjun Ma', 'Sarah M. Erfani', 'James Bailey']","Neural Architecture Search (NAS) has gained significant popularity as an
effective tool for designing high performance deep neural networks (DNNs). NAS
can be performed via policy gradient, evolutionary algorithms, differentiable
architecture search or tree-search methods. While significant progress has been
made for both policy gradient and differentiable architecture search,
tree-search methods have so far failed to achieve comparable accuracy or search
efficiency. In this paper, we formulate NAS as a Combinatorial Multi-Armed
Bandit (CMAB) problem (CMAB-NAS). This allows the decomposition of a large
search space into smaller blocks where tree-search methods can be applied more
effectively and efficiently. We further leverage a tree-based method called
Nested Monte-Carlo Search to tackle the CMAB-NAS problem. On CIFAR-10, our
approach discovers a cell structure that achieves a low error rate that is
comparable to the state-of-the-art, using only 0.58 GPU days, which is 20 times
faster than current tree-search methods. Moreover, the discovered structure
transfers well to large-scale datasets such as ImageNet.","10 pages, 7 figures",International Joint Conference on Neural Networks (IJCNN) 2021,,http://arxiv.org/abs/2101.00336v2,2021-04-24 14:13:15+00:00,2021-01-01 23:29:33+00:00,cs.LG,"['cs.LG', 'cs.CV', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2101.00336v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00336v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2101.00336v2
Minimum Viable Model Estimates for Machine Learning Projects,['John Hawkins'],"Prioritization of machine learning projects requires estimates of both the
potential ROI of the business case and the technical difficulty of building a
model with the required characteristics. In this work we present a technique
for estimating the minimum required performance characteristics of a predictive
model given a set of information about how it will be used. This technique will
result in robust, objective comparisons between potential projects. The
resulting estimates will allow data scientists and managers to evaluate whether
a proposed machine learning project is likely to succeed before any modelling
needs to be done.
  The technique has been implemented into the open source application MinViME
(Minimum Viable Model Estimator) which can be installed via the PyPI python
package management system, or downloaded directly from the GitHub repository.
Available at https://github.com/john-hawkins/MinViME","11 pages, 4 figures","Proceedings of the 6th International Conference on Computer
  Science, Engineering And Applications (CSEA 2020), December 18 ~ 19, pp.
  37-46, Volume 10, Number 18",10.5121/csit.2020.101803,http://arxiv.org/abs/2101.00346v1,2021-01-02 01:01:20+00:00,2021-01-02 01:01:20+00:00,cs.LG,"['cs.LG', 'cs.SE', '68T01', 'I.2.6; I.5.2']","[arxiv.Result.Link('http://dx.doi.org/10.5121/csit.2020.101803', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2101.00346v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00346v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2101.00346v1
Characterizing Fairness Over the Set of Good Models Under Selective Labels,"['Amanda Coston', 'Ashesh Rambachan', 'Alexandra Chouldechova']","Algorithmic risk assessments are used to inform decisions in a wide variety
of high-stakes settings. Often multiple predictive models deliver similar
overall performance but differ markedly in their predictions for individual
cases, an empirical phenomenon known as the ""Rashomon Effect."" These models may
have different properties over various groups, and therefore have different
predictive fairness properties. We develop a framework for characterizing
predictive fairness properties over the set of models that deliver similar
overall performance, or ""the set of good models."" Our framework addresses the
empirically relevant challenge of selectively labelled data in the setting
where the selection decision and outcome are unconfounded given the observed
data features. Our framework can be used to 1) replace an existing model with
one that has better fairness properties; or 2) audit for predictive bias. We
illustrate these uses cases on a real-world credit-scoring task and a
recidivism prediction task.",Added comparison methods to the empirical lending analysis,,,http://arxiv.org/abs/2101.00352v3,2021-05-01 00:21:59+00:00,2021-01-02 02:11:37+00:00,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2101.00352v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00352v3', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2101.00352v3
Integrated Optimization of Predictive and Prescriptive Tasks,"['Mehmet Kolcu', 'Alper E. Murat']","In traditional machine learning techniques, the degree of closeness between
true and predicted values generally measures the quality of predictions.
However, these learning algorithms do not consider prescription problems where
the predicted values will be used as input to decision problems. In this paper,
we efficiently leverage feature variables, and we propose a new framework
directly integrating predictive tasks under prescriptive tasks in order to
prescribe consistent decisions. We train the parameters of predictive algorithm
within a prescription problem via bilevel optimization techniques. We present
the structure of our method and demonstrate its performance using synthetic
data compared to classical methods like point-estimate-based, stochastic
optimization and recently developed machine learning based optimization
methods. In addition, we control generalization error using different penalty
approaches and optimize the integration over validation data set.",,,,http://arxiv.org/abs/2101.00354v1,2021-01-02 02:43:10+00:00,2021-01-02 02:43:10+00:00,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2101.00354v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00354v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2101.00354v1
Reinforcement Learning for Flexibility Design Problems,"['Yehua Wei', 'Lei Zhang', 'Ruiyi Zhang', 'Shijing Si', 'Hao Zhang', 'Lawrence Carin']","Flexibility design problems are a class of problems that appear in strategic
decision-making across industries, where the objective is to design a ($e.g.$,
manufacturing) network that affords flexibility and adaptivity. The underlying
combinatorial nature and stochastic objectives make flexibility design problems
challenging for standard optimization methods. In this paper, we develop a
reinforcement learning (RL) framework for flexibility design problems.
Specifically, we carefully design mechanisms with noisy exploration and
variance reduction to ensure empirical success and show the unique advantage of
RL in terms of fast-adaptation. Empirical results show that the RL-based method
consistently finds better solutions compared to classical heuristics.",,,,http://arxiv.org/abs/2101.00355v2,2021-01-18 14:35:06+00:00,2021-01-02 02:44:39+00:00,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2101.00355v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00355v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2101.00355v2
ORDisCo: Effective and Efficient Usage of Incremental Unlabeled Data for Semi-supervised Continual Learning,"['Liyuan Wang', 'Kuo Yang', 'Chongxuan Li', 'Lanqing Hong', 'Zhenguo Li', 'Jun Zhu']","Continual learning usually assumes the incoming data are fully labeled, which
might not be applicable in real applications. In this work, we consider
semi-supervised continual learning (SSCL) that incrementally learns from
partially labeled data. Observing that existing continual learning methods lack
the ability to continually exploit the unlabeled data, we propose deep Online
Replay with Discriminator Consistency (ORDisCo) to interdependently learn a
classifier with a conditional generative adversarial network (GAN), which
continually passes the learned data distribution to the classifier. In
particular, ORDisCo replays data sampled from the conditional generator to the
classifier in an online manner, exploiting unlabeled data in a time- and
storage-efficient way. Further, to explicitly overcome the catastrophic
forgetting of unlabeled data, we selectively stabilize parameters of the
discriminator that are important for discriminating the pairs of old unlabeled
data and their pseudo-labels predicted by the classifier. We extensively
evaluate ORDisCo on various semi-supervised learning benchmark datasets for
SSCL, and show that ORDisCo achieves significant performance improvement on
SVHN, CIFAR10 and Tiny-ImageNet, compared to strong baselines.",,CVPR 2021,,http://arxiv.org/abs/2101.00407v2,2021-04-09 01:57:03+00:00,2021-01-02 09:04:14+00:00,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2101.00407v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00407v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2101.00407v2
Representation Learning of Reconstructed Graphs Using Random Walk Graph Convolutional Network,"['Xing Li', 'Wei Wei', 'Xiangnan Feng', 'Zhiming Zheng']","Graphs are often used to organize data because of their simple topological
structure, and therefore play a key role in machine learning. And it turns out
that the low-dimensional embedded representation obtained by graph
representation learning are extremely useful in various typical tasks, such as
node classification, content recommendation and link prediction. However, the
existing methods mostly start from the microstructure (i.e., the edges) in the
graph, ignoring the mesoscopic structure (high-order local structure). Here, we
propose wGCN -- a novel framework that utilizes random walk to obtain the
node-specific mesoscopic structures of the graph, and utilizes these mesoscopic
structures to reconstruct the graph And organize the characteristic information
of the nodes. Our method can effectively generate node embeddings for
previously unseen data, which has been proven in a series of experiments
conducted on citation networks and social networks (our method has advantages
over baseline methods). We believe that combining high-order local structural
information can more efficiently explore the potential of the network, which
will greatly improve the learning efficiency of graph neural network and
promote the establishment of new learning models.","8 pages, 3 figures. arXiv admin note: text overlap with
  arXiv:2007.15838",,,http://arxiv.org/abs/2101.00417v1,2021-01-02 10:31:14+00:00,2021-01-02 10:31:14+00:00,cs.LG,['cs.LG'],"[arxiv.Result.Link('http://arxiv.org/abs/2101.00417v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00417v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2101.00417v1
"If You're Happy, Then You Know It: The Logic of Happiness... and Sadness","['Sanaz Azimipour', 'Pavel Naumov']","The article proposes a formal semantics of happiness and sadness modalities
in imperfect information setting. It shows that these modalities are not
definable through each other and gives a sound and complete axiomatization of
their properties.",,,,http://arxiv.org/abs/2101.00485v1,2021-01-02 17:42:19+00:00,2021-01-02 17:42:19+00:00,cs.AI,"['cs.AI', 'cs.LO']","[arxiv.Result.Link('http://arxiv.org/abs/2101.00485v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00485v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2101.00485v1
A Provably Efficient Algorithm for Linear Markov Decision Process with Low Switching Cost,"['Minbo Gao', 'Tianle Xie', 'Simon S. Du', 'Lin F. Yang']","Many real-world applications, such as those in medical domains,
recommendation systems, etc, can be formulated as large state space
reinforcement learning problems with only a small budget of the number of
policy changes, i.e., low switching cost. This paper focuses on the linear
Markov Decision Process (MDP) recently studied in [Yang et al 2019, Jin et al
2020] where the linear function approximation is used for generalization on the
large state space. We present the first algorithm for linear MDP with a low
switching cost. Our algorithm achieves an
$\widetilde{O}\left(\sqrt{d^3H^4K}\right)$ regret bound with a near-optimal
$O\left(d H\log K\right)$ global switching cost where $d$ is the feature
dimension, $H$ is the planning horizon and $K$ is the number of episodes the
agent plays. Our regret bound matches the best existing polynomial algorithm by
[Jin et al 2020] and our switching cost is exponentially smaller than theirs.
When specialized to tabular MDP, our switching cost bound improves those in
[Bai et al 2019, Zhang et al 20020]. We complement our positive result with an
$\Omega\left(dH/\log d\right)$ global switching cost lower bound for any
no-regret algorithm.",,,,http://arxiv.org/abs/2101.00494v1,2021-01-02 18:41:27+00:00,2021-01-02 18:41:27+00:00,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2101.00494v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00494v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2101.00494v1
